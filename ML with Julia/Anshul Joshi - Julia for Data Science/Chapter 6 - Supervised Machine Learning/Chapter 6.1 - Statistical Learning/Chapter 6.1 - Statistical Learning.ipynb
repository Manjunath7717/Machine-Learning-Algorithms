{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Statistical Learning with Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical learning refers to a vast set of tools for understanding data. \n",
    "\n",
    "These tools can be classified as supervised or unsupervised.\n",
    "\n",
    "### 1. Supervised Learning\n",
    "\n",
    "Supervised statistical learning involves building a statistical model for pre- dicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy.\n",
    "\n",
    "### 2. Unsupervised Learning\n",
    "\n",
    "Unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and struc- ture from such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to motivate our study of statistical learning, we begin with a simple example. Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting, the advertising budgets are input variables while sales is an output variable. The input variables are typically denoted using the symbol X, with a subscript to distinguish them. So X1 might be the TV budget, X2 the radio budget, and X3 the newspaper budget. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol Y . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, suppose that we observe a quantitative response Y and p different predictors, X1, X2, . . . , Xp. We assume that there is some relationship between Y and X = (X1,X2,...,Xp), which can be written in the very general form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Y = f(X)+e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f is some fixed but unknown function of X1, X2, . . . , Xp\n",
    "\n",
    "e is a random error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Estimate f?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main reasons that we may wish to estimate f: prediction and inference. We discuss each in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using\n",
    "\n",
    "      \n",
    "$\\hat{Y}$ = $\\hat{f}$(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{f}$ represents our estimate for f, and $\\hat{Y}$represents the resulting prediction for Y. In this setting, $\\hat{f}$ is often treated as a black box, in the sense\n",
    "that one is not typically concerned with the exact form of $\\hat{f}$, provided that\n",
    "it yields accurate predictions for Y ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a given estimate $\\hat{f}$ and a set of predictors X, which yields the\n",
    "prediction $\\hat{Y}$ = $\\hat{f}$(X). Assume for a moment that both $\\hat{f}$ and X are fixed.\n",
    "\n",
    "Then, it is easy to show that\n",
    "    \n",
    "    E(Y −Y)^2= E[f(X)+ε−f(X)]^2\n",
    "             = [f(X)−f(X)]2 + Var(ε)\n",
    "                Reducible      Irreducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where E(Y − $\\hat{Y}$ )^2 represents the average, or expected value, of the squared\n",
    "difference between the predicted and actual value of Y , and Var(ε) repre- sents the variance associated with the error term ε."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this book is on techniques for estimating f with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for Y . This bound is almost always unknown in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We are often interested in understanding the way that Y is affected as X1,...,Xp change. In this situation we wish to estimate f, but our goal is not necessarily to make predictions for Y. We instead want to understand the relationship between X and Y , or more specifically, to understand how Y changes as a function of X1, . . . , Xp.\n",
    "\n",
    "Now $\\hat{f}$  cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which predictors are associated with the response?\n",
    "2. What is the relationship between the response and each predictor?\n",
    "3. Can the relationship between Y and each predictor be adequately sum- marized using a linear equation, or is the relationship more compli- cated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Estimate f?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many linear and non-linear approaches for estimating f.\n",
    "    \n",
    "We will always assume that we have observed a set of n different\n",
    "data points. These observations are called the training data because we will use these\n",
    "observations to train, or teach, our method how to estimate f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let xij represent the value of the jth predictor, or input, for observation i, \n",
    "\n",
    "where\n",
    "i = 1,2,...,n and j = 1,2,...,p. \n",
    "\n",
    "Correspondingly, let yi represent the training data\n",
    "response variable for the ith observation. Then our training data consist of {(x1,y1),(x2,y2),...,(xn,yn)} where xi = (xi1,xi2,...,xip)^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical learning methods for this task can be character- ized as either parametric or non-parametric. We now briefly discuss these two types of approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameteric Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric methods involve a two-step model-based approach.\n",
    "1. First, we make an assumption about the functional form, or shape, of f.\n",
    "\n",
    "One very simple assumption is that f is linear in X:\n",
    "    \n",
    "    f(X) = β0 +β1X1 +β2X2 +...+βpXp.\n",
    "\n",
    "This is a linear model\n",
    "\n",
    "2. After a model has been selected, we need a procedure that uses the training data to fit or train the model.\n",
    "\n",
    "In the case of the linear model, we need to estimate the parameters β0,β1,...,βp. That is, we want to find values of these parameters such that\n",
    "\n",
    "    Y ≈β0 +β1X1 +β2X2 +...+βpXp.\n",
    "    \n",
    "The most common approach to fitting the model is referred to as (ordinary) least squares,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages:\n",
    " \n",
    " It does not matches the true unknown form of f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Non Parameteric\n",
    "\n",
    " Non-parametric methods do not make explicit assumptions about the func- tional form of f . Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvatages:\n",
    "Since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Trade-Off Between Prediction Accuracy and Model Interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate f.\n",
    "\n",
    "<img src=\"flex.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">\n",
    "\n",
    "We have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accurately  interpretability is not a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Versus Classification Problems\n",
    "\n",
    "\n",
    "Variables can be characterized as either quantitative or qualitative (also known as categorical). \n",
    "\n",
    "Quantitative variables take on numerical values.\n",
    "Examples include a person’s age, height, or income, the value of a house,\n",
    "and the price of a stock. \n",
    "\n",
    "In contrast, qualitative variables take on values in one of K different classes, or categories. \n",
    "Examples of qualitative\n",
    "variables include a person’s gender (male or female), the brand of prod-\n",
    "uct purchased (brand A, B, or C), whether a person defaults on a debt\n",
    "(yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute\n",
    "Lymphoblastic Leukemia, or No Leukemia).\n",
    "\n",
    "We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.\n",
    "\n",
    "Supervised Machine learning algorithms can be broadly divided into two types of algorithms \n",
    "<img src=\"SL.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">\n",
    "\n",
    "# Regression: \n",
    "\n",
    "Used to determine the the mathematical relationship between two ot more variable and the level of the dependency between varaiable.\n",
    "\n",
    "It is a Supervised Learning task where output is having continuous value.\n",
    "\n",
    "Example in above Figure B, Output – Wind Speed is not having any discrete value but is continuous in the particular range. The goal here is to predict a value as much closer to actual output value as our model can and then evaluation is done by calculating error value. The smaller the error the greater the accuracy of our regression model.\n",
    "\n",
    "| Regression Model         | Pros                               | Cons          |\n",
    "| :---                    |    :----:                          |          ---: |\n",
    "| Linear Regression       | Work on any size of data set gives info about the features | Linear Regression assumption   |\n",
    "| Polynomial Regression   | Work on any size of data and also on nonlinear        | Need to choose the right polynomial degree       |\n",
    "| SVR     | Easily adaptable, Nonlinear and not biased by outliers       | Compulsory to apply the features scaling   |\n",
    "| Decision Tree Regression   | Interpretable, no need for feature scaling and work on nonlinear        | Poor Result on small dataset       |\n",
    "| Random Forest Regression   | Powerful, accurate and work on nonlinear        | Overfitting, need to choose tree not interpretable      |\n",
    "\n",
    "\n",
    "# Classification\n",
    "\n",
    "These algorithms are used to classify data into predefined classes or labels\n",
    "\n",
    "| Classification Model         | Pros                               | Cons          |\n",
    "| :---                    |    :----:                          |          ---: |\n",
    "| Logistic Regression       |Probablilistics and provide significance of features  | Logistics Regression assumption   |\n",
    "| KNN   | simple fast and efficient        | Need to choose K       |\n",
    "| SVM     | Proformant and not biased by outliers       | Not appropriate for non linear    |\n",
    "| Naive Bayes    | Efficent, Nonlinear and Probalilistics       | Based on the assumption that feature have stats  |\n",
    "| Decision Tree Classification   | Interpretable, no need for feature scaling and work on nonlinear        | Poor Result on small dataset       |\n",
    "| Random Forest Classification   | Powerful, accurate and work on nonlinear        | Overfitting, need to choose tree and not interpretable      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key aims of this book is to introduce the reader to a wide range of statistical learning methods that extend far beyond the standard linear regression approach.\n",
    "\n",
    " Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Quality of Fit\n",
    "\n",
    "In order to evaluate the performance of a statistical learning method on\n",
    "a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent\n",
    "to which the predicted response value for a given observation is close to\n",
    "the true response value for that observation. In the regression setting, the\n",
    "most commonly-used measure is the mean squared error (MSE), given by\n",
    "        \n",
    " <img src=\"mse.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where  $\\hat{f}$(xi) is the prediction that  $\\hat{f}$ gives for the ith observation. \n",
    "\n",
    "The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen\n",
    "test data. We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ave(y0 −  $\\hat{f}$(x0))^2\n",
    "\n",
    " where (x0, y0) is a previously unseen test observation not used to train the statistical learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"abc.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the left- hand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve—note that in Figure, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of Figure. The horizontal dashed line indicates Var(ε), the irreducible error, which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline repre- sented by the blue curve is close to optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. \n",
    "\n",
    "Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can usually compute the training MSE with relative ease, but estimating test MSE is considerably more difficult because usually no test data are available. As the previous examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation, which is a method for estimating test MSE using the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Trade-Off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show that the expected test MSE, for a given value x0, can always be decomposed into the sum of three fundamental quantities: the variance of $\\hat{f}$(x0), the squared bias of $\\hat{f}$(x0) and the variance of the error terms ε. That is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E (y0 − $\\hat{f}$(x0))^2 = Var($\\hat{f}$(x0)) + [Bias($\\hat{f}$(x0))]^2 + Var(ε).\n",
    "\n",
    "E(y0 − $\\hat{f}$(x0))^2 defines the expected test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall 􏰊expected test MSE can be computed by averaging E(y0 − $\\hat{f}$(x0))^2  over all\n",
    "possible values of x0 in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias. Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below Var(ε), the irreducible error from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bias.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we mean by the variance and bias of a statistical learning method ?\n",
    "\n",
    "Variance refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\\hat{f}$. if a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$. In general, more flexible statistical methods have higher variance.\n",
    "\n",
    "Bias refers to the error that is introduced by approxi- mating a real-life problem, which may be extremely complicated, by a much simpler model. Linear regression results in high bias. The true f is very close to linear, and so given enough data, it should be possible for linear regression to produce an accurate estimate. Generally, more flexible methods result in less bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"var.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good test set performance of a statistical learning method re- quires low variance as well as low squared bias. The challenge lies in finding a method for which both the variance and the squared bias are low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression, a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. Linear regression has been around for a long time and is the topic of innumerable textbooks. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters of this book, linear regression is still a useful and widely used statistical learning method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Advertising Data\n",
    "\n",
    "Sales (in thousands of units) for a particular product as a function of advertis- ing budgets (in thousands of dollars) for TV, radio, and newspaper media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information would be useful in order to provide such a recommendation? Here are a few important questions that we might seek to address:\n",
    "\n",
    "1. Is there a relationship between advertising budget and sales?\n",
    "2. How strong is the relationship between advertising budget and sales?\n",
    "3. Which media contribute to sales?\n",
    "4. How accurately can we estimate the effect of each medium on sales?\n",
    "5. How accurately can we predict future sales?\n",
    "6. Is the relationship linear?\n",
    "7. Is there synergy among the advertising media?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression:\n",
    "\n",
    "Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a sin- gle predictor variable X. It assumes that there is approximately a linear relationship between X and Y . Mathematically, we can write this linear relationship as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Y ≈ β0 + β1X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  For example, \n",
    "\n",
    "X may represent TV advertising and Y may represent sales. Then we can regress sales onto TV by fitting the model\n",
    "\n",
    "    sales≈β0 +β1 ×TV.\n",
    "β0 and β1 are two unknown constants that represent\n",
    "the intercept and slope terms in the linear model. Together, β0 and β1 are\n",
    "known as the model coefficients or parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have used our training data to produce estimates $\\hat{β0}$ and $\\hat{β1}$ for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing\n",
    "    \n",
    "$\\hat{y}$ = $\\hat{β0}$ + $\\hat{β1}$x\n",
    "    \n",
    "where $\\hat{y}$ indicates a prediction of Y on the basis of X = x. Here we use a hat symbol, ˆ , to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Coefficients\n",
    "\n",
    "In practice, β0 and β1 are unknown. So before we can use (3.1) to make predictions, we must use data to estimate the coefficients. \n",
    "\n",
    "Let (x1,y1), (x2,y2),..., (xn,yn) represent n observation pairs, each of which consists of a measurement of X and a measurement of Y. However, by far the most common approach involves minimizing the least squares criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{y}$ = $\\hat{β0}$ + $\\hat{β1}$x be the prediction for Y based on the ith value of X.\n",
    "\n",
    "Then e = y −$\\hat{y}$ represents the ith residual this is the difference between the ith observed response value and the ith response value that is predicted by our linear model. We define the residual sum of squares (RSS) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rss.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares approach chooses $\\hat{β0}$ and $\\hat{β1}$ to minimize the RSS. Using some calculus, one can show that the minimizers are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"un.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where y ̄  and x ̄  are the sample means. In other words above equation defines the least squares coefficient estimates for simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the R2 statistic.\n",
    "\n",
    "## 1. Residual Standard Error\n",
    "Due to the presence of these error terms, even if we knew the true regression line (i.e. even if β0 and β1 were known), we would not be able to perfectly predict Y from X. The RSE is an estimate of the standard deviation of ε. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula\n",
    "            \n",
    "<img src=\"rse.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSE is considered a measure of the lack of fit of the model (3.5) to\n",
    "the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. $R^2$ Statistic\n",
    "\n",
    "The R2 statistic provides an alternative measure of fit. It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of Y.\n",
    "\n",
    "To calculate R2, we use the formula\n",
    "<img src=\"r.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where TSS = $\\sum􏰂(yi −  y ̄)^2$ is the total sum of squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ measures the proportion of variability in Y that can be explained using X. An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response; this might occur because the linear model is wrong, or the inherent error $σ^2$ is high, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 statistic is a measure of the linear relationship between X and Y. The squared correlation $r^2$ and the $R^2$ statistic are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ScikitLearn\n",
    "using PyCall\n",
    "using PyPlot, Printf\n",
    "using ScikitLearn.CrossValidation: train_test_split\n",
    "using Statistics, Random\n",
    "using DataFrames\n",
    "using CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._base.LinearRegression'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import linear_model: LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>YearsExperience</th><th>Salary</th></tr><tr><th></th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>30 rows × 2 columns</p><tr><th>1</th><td>1.1</td><td>39343.0</td></tr><tr><th>2</th><td>1.3</td><td>46205.0</td></tr><tr><th>3</th><td>1.5</td><td>37731.0</td></tr><tr><th>4</th><td>2.0</td><td>43525.0</td></tr><tr><th>5</th><td>2.2</td><td>39891.0</td></tr><tr><th>6</th><td>2.9</td><td>56642.0</td></tr><tr><th>7</th><td>3.0</td><td>60150.0</td></tr><tr><th>8</th><td>3.2</td><td>54445.0</td></tr><tr><th>9</th><td>3.2</td><td>64445.0</td></tr><tr><th>10</th><td>3.7</td><td>57189.0</td></tr><tr><th>11</th><td>3.9</td><td>63218.0</td></tr><tr><th>12</th><td>4.0</td><td>55794.0</td></tr><tr><th>13</th><td>4.0</td><td>56957.0</td></tr><tr><th>14</th><td>4.1</td><td>57081.0</td></tr><tr><th>15</th><td>4.5</td><td>61111.0</td></tr><tr><th>16</th><td>4.9</td><td>67938.0</td></tr><tr><th>17</th><td>5.1</td><td>66029.0</td></tr><tr><th>18</th><td>5.3</td><td>83088.0</td></tr><tr><th>19</th><td>5.9</td><td>81363.0</td></tr><tr><th>20</th><td>6.0</td><td>93940.0</td></tr><tr><th>21</th><td>6.8</td><td>91738.0</td></tr><tr><th>22</th><td>7.1</td><td>98273.0</td></tr><tr><th>23</th><td>7.9</td><td>101302.0</td></tr><tr><th>24</th><td>8.2</td><td>113812.0</td></tr><tr><th>25</th><td>8.7</td><td>109431.0</td></tr><tr><th>26</th><td>9.0</td><td>105582.0</td></tr><tr><th>27</th><td>9.5</td><td>116969.0</td></tr><tr><th>28</th><td>9.6</td><td>112635.0</td></tr><tr><th>29</th><td>10.3</td><td>122391.0</td></tr><tr><th>30</th><td>10.5</td><td>121872.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& YearsExperience & Salary\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.1 & 39343.0 \\\\\n",
       "\t2 & 1.3 & 46205.0 \\\\\n",
       "\t3 & 1.5 & 37731.0 \\\\\n",
       "\t4 & 2.0 & 43525.0 \\\\\n",
       "\t5 & 2.2 & 39891.0 \\\\\n",
       "\t6 & 2.9 & 56642.0 \\\\\n",
       "\t7 & 3.0 & 60150.0 \\\\\n",
       "\t8 & 3.2 & 54445.0 \\\\\n",
       "\t9 & 3.2 & 64445.0 \\\\\n",
       "\t10 & 3.7 & 57189.0 \\\\\n",
       "\t11 & 3.9 & 63218.0 \\\\\n",
       "\t12 & 4.0 & 55794.0 \\\\\n",
       "\t13 & 4.0 & 56957.0 \\\\\n",
       "\t14 & 4.1 & 57081.0 \\\\\n",
       "\t15 & 4.5 & 61111.0 \\\\\n",
       "\t16 & 4.9 & 67938.0 \\\\\n",
       "\t17 & 5.1 & 66029.0 \\\\\n",
       "\t18 & 5.3 & 83088.0 \\\\\n",
       "\t19 & 5.9 & 81363.0 \\\\\n",
       "\t20 & 6.0 & 93940.0 \\\\\n",
       "\t21 & 6.8 & 91738.0 \\\\\n",
       "\t22 & 7.1 & 98273.0 \\\\\n",
       "\t23 & 7.9 & 101302.0 \\\\\n",
       "\t24 & 8.2 & 113812.0 \\\\\n",
       "\t25 & 8.7 & 109431.0 \\\\\n",
       "\t26 & 9.0 & 105582.0 \\\\\n",
       "\t27 & 9.5 & 116969.0 \\\\\n",
       "\t28 & 9.6 & 112635.0 \\\\\n",
       "\t29 & 10.3 & 122391.0 \\\\\n",
       "\t30 & 10.5 & 121872.0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "30×2 DataFrame\n",
       "│ Row │ YearsExperience │ Salary   │\n",
       "│     │ \u001b[90mFloat64\u001b[39m         │ \u001b[90mFloat64\u001b[39m  │\n",
       "├─────┼─────────────────┼──────────┤\n",
       "│ 1   │ 1.1             │ 39343.0  │\n",
       "│ 2   │ 1.3             │ 46205.0  │\n",
       "│ 3   │ 1.5             │ 37731.0  │\n",
       "│ 4   │ 2.0             │ 43525.0  │\n",
       "│ 5   │ 2.2             │ 39891.0  │\n",
       "│ 6   │ 2.9             │ 56642.0  │\n",
       "│ 7   │ 3.0             │ 60150.0  │\n",
       "│ 8   │ 3.2             │ 54445.0  │\n",
       "│ 9   │ 3.2             │ 64445.0  │\n",
       "│ 10  │ 3.7             │ 57189.0  │\n",
       "⋮\n",
       "│ 20  │ 6.0             │ 93940.0  │\n",
       "│ 21  │ 6.8             │ 91738.0  │\n",
       "│ 22  │ 7.1             │ 98273.0  │\n",
       "│ 23  │ 7.9             │ 101302.0 │\n",
       "│ 24  │ 8.2             │ 113812.0 │\n",
       "│ 25  │ 8.7             │ 109431.0 │\n",
       "│ 26  │ 9.0             │ 105582.0 │\n",
       "│ 27  │ 9.5             │ 116969.0 │\n",
       "│ 28  │ 9.6             │ 112635.0 │\n",
       "│ 29  │ 10.3            │ 122391.0 │\n",
       "│ 30  │ 10.5            │ 121872.0 │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=DataFrame!(CSV.File(\"Salary_Data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: seriestype boxplot has been moved to StatsPlots.  To use: `Pkg.add(\"StatsPlots\"); using StatsPlots`\n",
      "└ @ Plots /Users/manjunathvhatkar/.julia/packages/Plots/cc8wh/src/args.jl:1093\n",
      "┌ Warning: seriestype boxplot has been moved to StatsPlots.  To use: `Pkg.add(\"StatsPlots\"); using StatsPlots`\n",
      "└ @ Plots /Users/manjunathvhatkar/.julia/packages/Plots/cc8wh/src/args.jl:1093\n"
     ]
    },
    {
     "ename": "ErrorException",
     "evalue": "The backend must not support the series type Val{:boxplot}, and there isn't a series recipe defined.",
     "output_type": "error",
     "traceback": [
      "The backend must not support the series type Val{:boxplot}, and there isn't a series recipe defined.",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] macro expansion at /Users/manjunathvhatkar/.julia/packages/Plots/cc8wh/src/series.jl:171 [inlined]",
      " [3] apply_recipe(::Plots.Attr, ::Type{Val{:boxplot}}, ::Base.OneTo{Int64}, ::Array{Float64,1}, ::Nothing) at /Users/manjunathvhatkar/.julia/packages/RecipesBase/G4s6f/src/RecipesBase.jl:279",
      " [4] _process_seriesrecipe(::Plots.Plot{Plots.GRBackend}, ::Plots.Attr) at /Users/manjunathvhatkar/.julia/packages/Plots/cc8wh/src/pipeline.jl:411",
      " [5] _plot!(::Plots.Plot{Plots.GRBackend}, ::Dict{Symbol,Any}, ::Tuple{CSV.Column{Float64,Float64}}) at /Users/manjunathvhatkar/.julia/packages/Plots/cc8wh/src/plot.jl:233",
      " [6] plot(::CSV.Column{Float64,Float64}; kw::Base.Iterators.Pairs{Symbol,Any,NTuple{4,Symbol},NamedTuple{(:title, :ylabel, :legend, :seriestype),Tuple{String,String,Bool,Symbol}}}) at /Users/manjunathvhatkar/.julia/packages/Plots/cc8wh/src/plot.jl:57",
      " [7] #boxplot#449 at /Users/manjunathvhatkar/.julia/packages/RecipesBase/G4s6f/src/RecipesBase.jl:393 [inlined]",
      " [8] top-level scope at In[5]:1"
     ]
    }
   ],
   "source": [
    "# Box Plot\n",
    "Plots.boxplot(df.Salary, title = \"Box Plot - Salary\", ylabel = \"Salary\", legend = false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "type NamedTuple has no field Salary",
     "output_type": "error",
     "traceback": [
      "type NamedTuple has no field Salary",
      "",
      "Stacktrace:",
      " [1] getproperty(::NamedTuple{(:y, :x),Tuple{Array{Float64,1},Array{Int64,1}}}, ::Symbol) at ./Base.jl:33",
      " [2] top-level scope at In[79]:1"
     ]
    }
   ],
   "source": [
    "# Density Plot\n",
    "density(df.Salary , title = \"Density Plot - Salary\", ylabel = \"Frequency\", xlabel = \"Salary\", legend = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "type NamedTuple has no field Salary",
     "output_type": "error",
     "traceback": [
      "type NamedTuple has no field Salary",
      "",
      "Stacktrace:",
      " [1] getproperty(::NamedTuple{(:y, :x),Tuple{Array{Float64,1},Array{Int64,1}}}, ::Symbol) at ./Base.jl:33",
      " [2] top-level scope at In[80]:1"
     ]
    }
   ],
   "source": [
    "Plots.histogram(df.Salary, title = \"Box Plot - Salary\", ylabel = \"Salary\", legend = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Analysis using Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "type NamedTuple has no field YearsExperience",
     "output_type": "error",
     "traceback": [
      "type NamedTuple has no field YearsExperience",
      "",
      "Stacktrace:",
      " [1] getproperty(::NamedTuple{(:y, :x),Tuple{Array{Float64,1},Array{Int64,1}}}, ::Symbol) at ./Base.jl:33",
      " [2] top-level scope at In[82]:1"
     ]
    }
   ],
   "source": [
    "# Correlation Analysis\n",
    "println(\"Correlation of Salary with YearsExperience Rate is \", cor(df.YearsExperience,df.Salary), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "type NamedTuple has no field YearsExperience",
     "output_type": "error",
     "traceback": [
      "type NamedTuple has no field YearsExperience",
      "",
      "Stacktrace:",
      " [1] getproperty(::NamedTuple{(:y, :x),Tuple{Array{Float64,1},Array{Int64,1}}}, ::Symbol) at ./Base.jl:33",
      " [2] top-level scope at In[83]:1"
     ]
    }
   ],
   "source": [
    "# Scatter plot\n",
    "train_plot = Plots.scatter(df.YearsExperience,df.Salary, title = \"Scatter Plot Salary vs YearsExperience Rate\", ylabel = \"Salary\", xlabel = \"YearsExperience\",legend = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching getindex(::NamedTuple{(:y, :x),Tuple{Array{Float64,1},Array{Int64,1}}}, ::Array{Symbol,1})\nClosest candidates are:\n  getindex(::NamedTuple, !Matched::Int64) at namedtuple.jl:94\n  getindex(::NamedTuple, !Matched::Symbol) at namedtuple.jl:95",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching getindex(::NamedTuple{(:y, :x),Tuple{Array{Float64,1},Array{Int64,1}}}, ::Array{Symbol,1})\nClosest candidates are:\n  getindex(::NamedTuple, !Matched::Int64) at namedtuple.jl:94\n  getindex(::NamedTuple, !Matched::Symbol) at namedtuple.jl:95",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[84]:1"
     ]
    }
   ],
   "source": [
    "X = convert(Array, df[[:YearsExperience]])\n",
    "y = convert(Array, df[:Salary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError('Found input variables with inconsistent numbers of samples: [30, 506]')\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\", line 2127, in train_test_split\n    arrays = indexable(*arrays)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 293, in indexable\n    check_consistent_length(*result)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 257, in check_consistent_length\n    \" samples: %r\" % [int(l) for l in lengths])\n",
     "output_type": "error",
     "traceback": [
      "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError('Found input variables with inconsistent numbers of samples: [30, 506]')\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\", line 2127, in train_test_split\n    arrays = indexable(*arrays)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 293, in indexable\n    check_consistent_length(*result)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 257, in check_consistent_length\n    \" samples: %r\" % [int(l) for l in lengths])\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:60 [inlined]",
      " [2] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:64 [inlined]",
      " [3] _handle_error(::String) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:81",
      " [4] macro expansion at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:95 [inlined]",
      " [5] #110 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 [inlined]",
      " [6] disable_sigint at ./c.jl:446 [inlined]",
      " [7] __pycall! at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:42 [inlined]",
      " [8] _pycall!(::PyObject, ::PyObject, ::Tuple{Array{Float64,1},Array{Float64,1}}, ::Int64, ::PyObject) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:29",
      " [9] _pycall!(::PyObject, ::PyObject, ::Tuple{Array{Float64,1},Array{Float64,1}}, ::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol},NamedTuple{(:test_size, :random_state),Tuple{Float64,Int64}}}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:11",
      " [10] #_#117 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86 [inlined]",
      " [11] train_test_split(::Array{Float64,1}, ::Vararg{Array{Float64,1},N} where N; kwargs::Base.Iterators.Pairs{Symbol,Real,Tuple{Symbol,Symbol},NamedTuple{(:test_size, :random_state),Tuple{Float64,Int64}}}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/cross_validation.jl:667",
      " [12] top-level scope at In[85]:1"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LinearRegression()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError(\"could not convert string to float: 'versicolor'\")\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 515, in fit\n    return_mean=True)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 136, in _preprocess_data\n    y = np.asarray(y, dtype=X.dtype)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n",
     "output_type": "error",
     "traceback": [
      "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError(\"could not convert string to float: 'versicolor'\")\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 515, in fit\n    return_mean=True)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 136, in _preprocess_data\n    y = np.asarray(y, dtype=X.dtype)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:60 [inlined]",
      " [2] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:64 [inlined]",
      " [3] _handle_error(::String) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:81",
      " [4] macro expansion at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:95 [inlined]",
      " [5] #110 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 [inlined]",
      " [6] disable_sigint at ./c.jl:446 [inlined]",
      " [7] __pycall! at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:42 [inlined]",
      " [8] _pycall!(::PyObject, ::PyObject, ::Tuple{Array{Float64,2},Array{String,1}}, ::Int64, ::Ptr{Nothing}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:29",
      " [9] _pycall!(::PyObject, ::PyObject, ::Tuple{Array{Float64,2},Array{String,1}}, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{,Tuple{}}}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:11",
      " [10] (::PyObject)(::Array{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{,Tuple{}}}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86",
      " [11] (::PyObject)(::Array{Float64,2}, ::Vararg{Any,N} where N) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86",
      " [12] fit!(::PyObject, ::Array{Float64,2}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{,Tuple{}}}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/Skcore.jl:102",
      " [13] fit!(::PyObject, ::Array{Float64,2}, ::Array{String,1}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/Skcore.jl:102",
      " [14] top-level scope at In[87]:1"
     ]
    }
   ],
   "source": [
    "fit!(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError('Expected 2D array, got 1D array instead:\\narray=[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\\n 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111\\n 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\\n 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323\\n 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\\n 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\\n 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141\\n 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\\n 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\\n 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596\\n 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\\n 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172\\n 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\\n 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384\\n 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899\\n 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\\n 0.96969697 0.97979798 0.98989899 1.        ].\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.')\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 236, in predict\n    return self._decision_function(X)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 218, in _decision_function\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 624, in check_array\n    \"if it contains a single sample.\".format(array))\n",
     "output_type": "error",
     "traceback": [
      "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError('Expected 2D array, got 1D array instead:\\narray=[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\\n 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111\\n 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\\n 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323\\n 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\\n 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\\n 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141\\n 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\\n 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\\n 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596\\n 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\\n 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172\\n 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\\n 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384\\n 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899\\n 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\\n 0.96969697 0.97979798 0.98989899 1.        ].\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.')\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 236, in predict\n    return self._decision_function(X)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 218, in _decision_function\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 624, in check_array\n    \"if it contains a single sample.\".format(array))\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:60 [inlined]",
      " [2] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:64 [inlined]",
      " [3] _handle_error(::String) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:81",
      " [4] macro expansion at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:95 [inlined]",
      " [5] #110 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 [inlined]",
      " [6] disable_sigint at ./c.jl:446 [inlined]",
      " [7] __pycall! at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:42 [inlined]",
      " [8] _pycall!(::PyObject, ::PyObject, ::Tuple{StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}, ::Int64, ::Ptr{Nothing}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:29",
      " [9] _pycall! at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:11 [inlined]",
      " [10] #_#117 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86 [inlined]",
      " [11] (::PyObject)(::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86",
      " [12] predict(::PyObject, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{,Tuple{}}}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/Skcore.jl:102",
      " [13] predict(::PyObject, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/Skcore.jl:102",
      " [14] top-level scope at In[88]:1"
     ]
    }
   ],
   "source": [
    "y=predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError('Expected 2D array, got 1D array instead:\\narray=[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\\n 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111\\n 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\\n 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323\\n 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\\n 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\\n 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141\\n 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\\n 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\\n 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596\\n 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\\n 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172\\n 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\\n 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384\\n 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899\\n 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\\n 0.96969697 0.97979798 0.98989899 1.        ].\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.')\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 236, in predict\n    return self._decision_function(X)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 218, in _decision_function\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 624, in check_array\n    \"if it contains a single sample.\".format(array))\n",
     "output_type": "error",
     "traceback": [
      "PyError ($(Expr(:escape, :(ccall(#= /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'ValueError'>\nValueError('Expected 2D array, got 1D array instead:\\narray=[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\\n 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111\\n 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\\n 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323\\n 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\\n 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\\n 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141\\n 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\\n 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\\n 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596\\n 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\\n 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172\\n 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\\n 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384\\n 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899\\n 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\\n 0.96969697 0.97979798 0.98989899 1.        ].\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.')\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 236, in predict\n    return self._decision_function(X)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\", line 218, in _decision_function\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/manjunathvhatkar/.julia/conda/3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 624, in check_array\n    \"if it contains a single sample.\".format(array))\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:60 [inlined]",
      " [2] pyerr_check at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:64 [inlined]",
      " [3] _handle_error(::String) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:81",
      " [4] macro expansion at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/exception.jl:95 [inlined]",
      " [5] #110 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 [inlined]",
      " [6] disable_sigint at ./c.jl:446 [inlined]",
      " [7] __pycall! at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:42 [inlined]",
      " [8] _pycall!(::PyObject, ::PyObject, ::Tuple{StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}, ::Int64, ::Ptr{Nothing}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:29",
      " [9] _pycall! at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:11 [inlined]",
      " [10] #_#117 at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86 [inlined]",
      " [11] (::PyObject)(::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) at /Users/manjunathvhatkar/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:86",
      " [12] predict(::PyObject, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{,Tuple{}}}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/Skcore.jl:102",
      " [13] predict(::PyObject, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) at /Users/manjunathvhatkar/.julia/packages/ScikitLearn/Kn82b/src/Skcore.jl:102",
      " [14] top-level scope at In[89]:1"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "ypredicted_test = predict(model, X_test)\n",
    "ypredicted_train = predict(model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9-element Array{Float64,1}:\n",
       "      9.528230958965257e6\n",
       " 635340.6536746444\n",
       "      6.5180798589208424e7\n",
       "   4148.694134326295\n",
       "      1.6106819206122393e6\n",
       "      1.4867789857604443e6\n",
       "      1.6007198318693426e7\n",
       "      7.097113010618044e7\n",
       "      4.490640098026753e7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Performance DataFrame (compute squared error)\n",
    "performance_testdf = DataFrame(y_test=y_test, y_predicted = ypredicted_test)\n",
    "performance_testdf.error = performance_testdf[!,:y_test] - performance_testdf[!,:y_predicted]\n",
    "performance_testdf.error_sq = performance_testdf.error.*performance_testdf.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21-element Array{Float64,1}:\n",
       " 334713.7360185252\n",
       "      7.397589075116016e6\n",
       "      7.200751012035824e7\n",
       "      5.951771538995937e7\n",
       "      6.064620178975702e7\n",
       "      1.056879834802765e8\n",
       "      1.7182803033147797e6\n",
       "      5.269698622199937e7\n",
       "      1.0164203797752442e7\n",
       "      2.800340971623836e7\n",
       "      5.593493498432483e7\n",
       " 409514.1545658758\n",
       "      1.2102290179964925e8\n",
       "      1.7819864209399346e7\n",
       "      5.22226453030587e6\n",
       "      2.956904803073682e7\n",
       "      3.892389310417044e6\n",
       "      5.14981890842139e6\n",
       "      2.5378960597062133e7\n",
       "      2.2133935273590993e7\n",
       "      5.272845983980899e7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Performance DataFrame (compute squared error)\n",
    "performance_traindf = DataFrame(y_train = y_train, y_predicted = ypredicted_train)\n",
    "performance_traindf.error = performance_traindf[!,:y_train] - performance_traindf[!,:y_predicted]\n",
    "performance_traindf.error_sq = performance_traindf.error.*performance_traindf.error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapetest (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAPE function defination\n",
    "function mapetest(performance_df)\n",
    "    mape = mean(abs.(performance_df.error./performance_df.y_test))\n",
    "    return mape\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapetrain (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAPE function defination\n",
    "function mapetrain(performance_df)\n",
    "    mape = mean(abs.(performance_df.error./performance_df.y_train))\n",
    "    return mape\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE function defination\n",
    "function rmse(performance_df)\n",
    "    rmse = sqrt(mean(performance_df.error.*performance_df.error))\n",
    "    return rmse\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute test error: 3737.417861878896\n",
      "\n",
      "Mean Aboslute Percentage test error: 0.05777087699629181\n",
      "\n",
      "Root mean square test error: 4834.260936361728\n",
      "\n",
      "Mean square test error: 2.3370078800832972e7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Error\n",
    "println(\"Mean Absolute test error: \",mean(abs.(performance_testdf.error)), \"\\n\")\n",
    "println(\"Mean Aboslute Percentage test error: \",mapetest(performance_testdf), \"\\n\")\n",
    "println(\"Root mean square test error: \",rmse(performance_testdf), \"\\n\")\n",
    "println(\"Mean square test error: \",mean(performance_testdf.error_sq), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train error: 5091.778618459526\n",
      "\n",
      "Mean Absolute Percentage train error: 0.07790527668082674\n",
      "\n",
      "Root mean square train error: 5925.8782160841465\n",
      "\n",
      "Mean square train error: 3.511603263186063e7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train  Error\n",
    "println(\"Mean train error: \",mean(abs.(performance_traindf.error)), \"\\n\")\n",
    "println(\"Mean Absolute Percentage train error: \",mapetrain(performance_traindf), \"\\n\")\n",
    "println(\"Root mean square train error: \",rmse(performance_traindf), \"\\n\")\n",
    "println(\"Mean square train error: \",mean(performance_traindf.error_sq), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip6200)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6201\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip6200)\" d=\"\n",
       "M363.188 1425.62 L2352.76 1425.62 L2352.76 121.675 L363.188 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6202\">\n",
       "    <rect x=\"363\" y=\"121\" width=\"1991\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  794.886,1425.62 794.886,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1264.12,1425.62 1264.12,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1733.36,1425.62 1733.36,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2202.6,1425.62 2202.6,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1378.92 2352.76,1378.92 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1080.81 2352.76,1080.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,782.702 2352.76,782.702 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,484.592 2352.76,484.592 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,186.482 2352.76,186.482 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 363.188,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  794.886,1425.62 794.886,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1264.12,1425.62 1264.12,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1733.36,1425.62 1733.36,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2202.6,1425.62 2202.6,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1378.92 387.062,1378.92 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1080.81 387.062,1080.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,782.702 387.062,782.702 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,484.592 387.062,484.592 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,186.482 387.062,186.482 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 794.886, 1479.62)\" x=\"794.886\" y=\"1479.62\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1264.12, 1479.62)\" x=\"1264.12\" y=\"1479.62\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1733.36, 1479.62)\" x=\"1733.36\" y=\"1479.62\">15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2202.6, 1479.62)\" x=\"2202.6\" y=\"1479.62\">20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1402.65)\" x=\"168.903\" y=\"1402.65\">4.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1375.24)\" x=\"317.443\" y=\"1375.24\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1104.54)\" x=\"168.903\" y=\"1104.54\">6.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1077.13)\" x=\"317.443\" y=\"1077.13\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 806.43)\" x=\"168.903\" y=\"806.43\">8.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 779.019)\" x=\"317.443\" y=\"779.019\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 508.32)\" x=\"168.903\" y=\"508.32\">1.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 480.909)\" x=\"317.443\" y=\"480.909\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 210.21)\" x=\"168.903\" y=\"210.21\">1.2×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 182.799)\" x=\"317.443\" y=\"182.799\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 73.2)\" x=\"1357.97\" y=\"73.2\">Scatter Plot Salary vs YearsExperience Rate</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 1559.48)\" x=\"1357.97\" y=\"1559.48\">YearsExperience</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">Salary</text>\n",
       "</g>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"419.496\" cy=\"465.185\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"513.344\" cy=\"1130.86\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"607.191\" cy=\"990.947\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"701.039\" cy=\"1014.56\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"794.886\" cy=\"1064.25\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"888.734\" cy=\"278.717\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"982.582\" cy=\"607.741\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1076.43\" cy=\"1286.43\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1170.28\" cy=\"158.579\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1264.12\" cy=\"1078.58\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1357.97\" cy=\"1380.55\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1451.82\" cy=\"762.386\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1545.67\" cy=\"574.919\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1639.51\" cy=\"1122.71\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1733.36\" cy=\"1163.61\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1827.21\" cy=\"401.39\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"1921.06\" cy=\"1326.38\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"2014.9\" cy=\"1388.71\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"2108.75\" cy=\"510.334\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"2202.6\" cy=\"962.492\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6202)\" cx=\"2296.45\" cy=\"1126.17\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "train_plot = Plots.scatter(y_train, title = \"Scatter Plot Salary vs YearsExperience Rate\", ylabel = \"Salary\", xlabel = \"YearsExperience\",legend = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6600\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip6600)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6601\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip6600)\" d=\"\n",
       "M363.188 1425.62 L2352.76 1425.62 L2352.76 121.675 L363.188 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6602\">\n",
       "    <rect x=\"363\" y=\"121\" width=\"1991\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  794.886,1425.62 794.886,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1264.12,1425.62 1264.12,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1733.36,1425.62 1733.36,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2202.6,1425.62 2202.6,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1347.8 2352.76,1347.8 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1068.18 2352.76,1068.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,788.563 2352.76,788.563 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,508.944 2352.76,508.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,229.325 2352.76,229.325 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 363.188,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  794.886,1425.62 794.886,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1264.12,1425.62 1264.12,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1733.36,1425.62 1733.36,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2202.6,1425.62 2202.6,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1347.8 387.062,1347.8 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1068.18 387.062,1068.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,788.563 387.062,788.563 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,508.944 387.062,508.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,229.325 387.062,229.325 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 794.886, 1479.62)\" x=\"794.886\" y=\"1479.62\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1264.12, 1479.62)\" x=\"1264.12\" y=\"1479.62\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1733.36, 1479.62)\" x=\"1733.36\" y=\"1479.62\">15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2202.6, 1479.62)\" x=\"2202.6\" y=\"1479.62\">20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1371.53)\" x=\"168.903\" y=\"1371.53\">4.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1344.12)\" x=\"317.443\" y=\"1344.12\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1091.91)\" x=\"168.903\" y=\"1091.91\">6.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1064.5)\" x=\"317.443\" y=\"1064.5\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 812.291)\" x=\"168.903\" y=\"812.291\">8.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 784.88)\" x=\"317.443\" y=\"784.88\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 532.672)\" x=\"168.903\" y=\"532.672\">1.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 505.261)\" x=\"317.443\" y=\"505.261\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 253.052)\" x=\"168.903\" y=\"253.052\">1.2×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 225.642)\" x=\"317.443\" y=\"225.642\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 73.2)\" x=\"1357.97\" y=\"73.2\">Scatter Plot Salary vs YearsExperience Rate</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 1559.48)\" x=\"1357.97\" y=\"1559.48\">YearsExperience</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">Salary</text>\n",
       "</g>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"419.496\" cy=\"490.741\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"513.344\" cy=\"1115.13\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"607.191\" cy=\"983.891\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"701.039\" cy=\"1006.04\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"794.886\" cy=\"1052.65\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"888.734\" cy=\"315.839\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"982.582\" cy=\"624.455\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1076.43\" cy=\"1261.05\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1170.28\" cy=\"203.152\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1264.12\" cy=\"1066.09\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1357.97\" cy=\"1349.33\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1451.82\" cy=\"769.507\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1545.67\" cy=\"593.669\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1639.51\" cy=\"1107.48\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1733.36\" cy=\"1145.85\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1827.21\" cy=\"430.902\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"1921.06\" cy=\"1298.52\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"2014.9\" cy=\"1356.99\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"2108.75\" cy=\"533.089\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"2202.6\" cy=\"957.202\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip6602)\" cx=\"2296.45\" cy=\"1110.73\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<polyline clip-path=\"url(#clip6602)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  419.496,498.829 513.344,1153.16 607.191,865.253 701.039,1113.9 794.886,943.772 888.734,459.57 982.582,642.781 1076.43,1362.54 1170.28,158.579 1264.12,1140.07 \n",
       "  1357.97,1244.76 1451.82,760.56 1545.67,747.474 1639.51,1048.46 1733.36,1113.9 1827.21,354.877 1921.06,1270.94 2014.9,1388.71 2108.75,603.522 2202.6,891.426 \n",
       "  2296.45,1009.2 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot!(ypredicted_train, label=\"fit_exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip7000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip7000)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7001\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip7000)\" d=\"\n",
       "M363.188 1425.62 L2352.76 1425.62 L2352.76 121.675 L363.188 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7002\">\n",
       "    <rect x=\"363\" y=\"121\" width=\"1991\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  654.115,1425.62 654.115,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1123.35,1425.62 1123.35,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1592.59,1425.62 1592.59,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2061.83,1425.62 2061.83,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1355.75 2352.76,1355.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1065.14 2352.76,1065.14 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,774.533 2352.76,774.533 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,483.927 2352.76,483.927 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,193.321 2352.76,193.321 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 363.188,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  654.115,1425.62 654.115,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1123.35,1425.62 1123.35,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1592.59,1425.62 1592.59,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2061.83,1425.62 2061.83,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1355.75 387.062,1355.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1065.14 387.062,1065.14 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,774.533 387.062,774.533 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,483.927 387.062,483.927 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,193.321 387.062,193.321 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 654.115, 1479.62)\" x=\"654.115\" y=\"1479.62\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1123.35, 1479.62)\" x=\"1123.35\" y=\"1479.62\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1592.59, 1479.62)\" x=\"1592.59\" y=\"1479.62\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2061.83, 1479.62)\" x=\"2061.83\" y=\"1479.62\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1379.47)\" x=\"168.903\" y=\"1379.47\">4.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1352.06)\" x=\"317.443\" y=\"1352.06\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1088.87)\" x=\"168.903\" y=\"1088.87\">6.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1061.46)\" x=\"317.443\" y=\"1061.46\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 798.261)\" x=\"168.903\" y=\"798.261\">8.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 770.85)\" x=\"317.443\" y=\"770.85\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 507.655)\" x=\"168.903\" y=\"507.655\">1.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 480.244)\" x=\"317.443\" y=\"480.244\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 217.049)\" x=\"168.903\" y=\"217.049\">1.2×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 189.638)\" x=\"317.443\" y=\"189.638\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 73.2)\" x=\"1357.97\" y=\"73.2\">Scatter Plot Salary vs YearsExperience Rate</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 1559.48)\" x=\"1357.97\" y=\"1559.48\">YearsExperience</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">Salary</text>\n",
       "</g>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"419.496\" cy=\"1388.71\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"654.115\" cy=\"158.579\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"888.734\" cy=\"1107.55\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"1123.35\" cy=\"1018.38\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"1357.97\" cy=\"237.362\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"1592.59\" cy=\"346.892\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"1827.21\" cy=\"300.337\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"2061.83\" cy=\"1126.25\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7002)\" cx=\"2296.45\" cy=\"729.664\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "train_plot = Plots.scatter(y_test, title = \"Scatter Plot Salary vs YearsExperience Rate\", ylabel = \"Salary\", xlabel = \"YearsExperience\",legend = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip7400\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip7400)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7401\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip7400)\" d=\"\n",
       "M363.188 1425.62 L2352.76 1425.62 L2352.76 121.675 L363.188 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7402\">\n",
       "    <rect x=\"363\" y=\"121\" width=\"1991\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  654.115,1425.62 654.115,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1123.35,1425.62 1123.35,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1592.59,1425.62 1592.59,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2061.83,1425.62 2061.83,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1356.05 2352.76,1356.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,1068.16 2352.76,1068.16 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,780.262 2352.76,780.262 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,492.366 2352.76,492.366 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  363.188,204.471 2352.76,204.471 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1425.62 363.188,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  654.115,1425.62 654.115,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1123.35,1425.62 1123.35,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1592.59,1425.62 1592.59,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2061.83,1425.62 2061.83,1409.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1356.05 387.062,1356.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,1068.16 387.062,1068.16 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,780.262 387.062,780.262 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,492.366 387.062,492.366 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  363.188,204.471 387.062,204.471 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 654.115, 1479.62)\" x=\"654.115\" y=\"1479.62\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1123.35, 1479.62)\" x=\"1123.35\" y=\"1479.62\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1592.59, 1479.62)\" x=\"1592.59\" y=\"1479.62\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2061.83, 1479.62)\" x=\"2061.83\" y=\"1479.62\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1379.78)\" x=\"168.903\" y=\"1379.78\">4.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1352.37)\" x=\"317.443\" y=\"1352.37\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 1091.88)\" x=\"168.903\" y=\"1091.88\">6.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 1064.47)\" x=\"317.443\" y=\"1064.47\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 803.989)\" x=\"168.903\" y=\"803.989\">8.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 776.579)\" x=\"317.443\" y=\"776.579\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 516.094)\" x=\"168.903\" y=\"516.094\">1.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 488.684)\" x=\"317.443\" y=\"488.684\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 168.903, 228.199)\" x=\"168.903\" y=\"228.199\">1.2×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 317.443, 200.788)\" x=\"317.443\" y=\"200.788\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 73.2)\" x=\"1357.97\" y=\"73.2\">Scatter Plot Salary vs YearsExperience Rate</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1357.97, 1559.48)\" x=\"1357.97\" y=\"1559.48\">YearsExperience</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">Salary</text>\n",
       "</g>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"419.496\" cy=\"1388.71\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"654.115\" cy=\"170.053\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"888.734\" cy=\"1110.18\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"1123.35\" cy=\"1021.83\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"1357.97\" cy=\"248.101\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"1592.59\" cy=\"356.609\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"1827.21\" cy=\"310.488\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"2061.83\" cy=\"1128.7\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip7402)\" cx=\"2296.45\" cy=\"735.811\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<polyline clip-path=\"url(#clip7402)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  419.496,1344.28 654.115,158.579 888.734,993.96 1123.35,1020.91 1357.97,266.37 1592.59,374.161 1827.21,252.896 2061.83,1007.43 2296.45,832.273 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot!(ypredicted_test, label=\"fit_exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using GLM.predict in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using GLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>YearsExperience</th><th>Salary</th></tr><tr><th></th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>30 rows × 2 columns</p><tr><th>1</th><td>1.1</td><td>39343.0</td></tr><tr><th>2</th><td>1.3</td><td>46205.0</td></tr><tr><th>3</th><td>1.5</td><td>37731.0</td></tr><tr><th>4</th><td>2.0</td><td>43525.0</td></tr><tr><th>5</th><td>2.2</td><td>39891.0</td></tr><tr><th>6</th><td>2.9</td><td>56642.0</td></tr><tr><th>7</th><td>3.0</td><td>60150.0</td></tr><tr><th>8</th><td>3.2</td><td>54445.0</td></tr><tr><th>9</th><td>3.2</td><td>64445.0</td></tr><tr><th>10</th><td>3.7</td><td>57189.0</td></tr><tr><th>11</th><td>3.9</td><td>63218.0</td></tr><tr><th>12</th><td>4.0</td><td>55794.0</td></tr><tr><th>13</th><td>4.0</td><td>56957.0</td></tr><tr><th>14</th><td>4.1</td><td>57081.0</td></tr><tr><th>15</th><td>4.5</td><td>61111.0</td></tr><tr><th>16</th><td>4.9</td><td>67938.0</td></tr><tr><th>17</th><td>5.1</td><td>66029.0</td></tr><tr><th>18</th><td>5.3</td><td>83088.0</td></tr><tr><th>19</th><td>5.9</td><td>81363.0</td></tr><tr><th>20</th><td>6.0</td><td>93940.0</td></tr><tr><th>21</th><td>6.8</td><td>91738.0</td></tr><tr><th>22</th><td>7.1</td><td>98273.0</td></tr><tr><th>23</th><td>7.9</td><td>101302.0</td></tr><tr><th>24</th><td>8.2</td><td>113812.0</td></tr><tr><th>25</th><td>8.7</td><td>109431.0</td></tr><tr><th>26</th><td>9.0</td><td>105582.0</td></tr><tr><th>27</th><td>9.5</td><td>116969.0</td></tr><tr><th>28</th><td>9.6</td><td>112635.0</td></tr><tr><th>29</th><td>10.3</td><td>122391.0</td></tr><tr><th>30</th><td>10.5</td><td>121872.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& YearsExperience & Salary\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.1 & 39343.0 \\\\\n",
       "\t2 & 1.3 & 46205.0 \\\\\n",
       "\t3 & 1.5 & 37731.0 \\\\\n",
       "\t4 & 2.0 & 43525.0 \\\\\n",
       "\t5 & 2.2 & 39891.0 \\\\\n",
       "\t6 & 2.9 & 56642.0 \\\\\n",
       "\t7 & 3.0 & 60150.0 \\\\\n",
       "\t8 & 3.2 & 54445.0 \\\\\n",
       "\t9 & 3.2 & 64445.0 \\\\\n",
       "\t10 & 3.7 & 57189.0 \\\\\n",
       "\t11 & 3.9 & 63218.0 \\\\\n",
       "\t12 & 4.0 & 55794.0 \\\\\n",
       "\t13 & 4.0 & 56957.0 \\\\\n",
       "\t14 & 4.1 & 57081.0 \\\\\n",
       "\t15 & 4.5 & 61111.0 \\\\\n",
       "\t16 & 4.9 & 67938.0 \\\\\n",
       "\t17 & 5.1 & 66029.0 \\\\\n",
       "\t18 & 5.3 & 83088.0 \\\\\n",
       "\t19 & 5.9 & 81363.0 \\\\\n",
       "\t20 & 6.0 & 93940.0 \\\\\n",
       "\t21 & 6.8 & 91738.0 \\\\\n",
       "\t22 & 7.1 & 98273.0 \\\\\n",
       "\t23 & 7.9 & 101302.0 \\\\\n",
       "\t24 & 8.2 & 113812.0 \\\\\n",
       "\t25 & 8.7 & 109431.0 \\\\\n",
       "\t26 & 9.0 & 105582.0 \\\\\n",
       "\t27 & 9.5 & 116969.0 \\\\\n",
       "\t28 & 9.6 & 112635.0 \\\\\n",
       "\t29 & 10.3 & 122391.0 \\\\\n",
       "\t30 & 10.5 & 121872.0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "30×2 DataFrame\n",
       "│ Row │ YearsExperience │ Salary   │\n",
       "│     │ \u001b[90mFloat64\u001b[39m         │ \u001b[90mFloat64\u001b[39m  │\n",
       "├─────┼─────────────────┼──────────┤\n",
       "│ 1   │ 1.1             │ 39343.0  │\n",
       "│ 2   │ 1.3             │ 46205.0  │\n",
       "│ 3   │ 1.5             │ 37731.0  │\n",
       "│ 4   │ 2.0             │ 43525.0  │\n",
       "│ 5   │ 2.2             │ 39891.0  │\n",
       "│ 6   │ 2.9             │ 56642.0  │\n",
       "│ 7   │ 3.0             │ 60150.0  │\n",
       "│ 8   │ 3.2             │ 54445.0  │\n",
       "│ 9   │ 3.2             │ 64445.0  │\n",
       "│ 10  │ 3.7             │ 57189.0  │\n",
       "⋮\n",
       "│ 20  │ 6.0             │ 93940.0  │\n",
       "│ 21  │ 6.8             │ 91738.0  │\n",
       "│ 22  │ 7.1             │ 98273.0  │\n",
       "│ 23  │ 7.9             │ 101302.0 │\n",
       "│ 24  │ 8.2             │ 113812.0 │\n",
       "│ 25  │ 8.7             │ 109431.0 │\n",
       "│ 26  │ 9.0             │ 105582.0 │\n",
       "│ 27  │ 9.5             │ 116969.0 │\n",
       "│ 28  │ 9.6             │ 112635.0 │\n",
       "│ 29  │ 10.3            │ 122391.0 │\n",
       "│ 30  │ 10.5            │ 121872.0 │"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "Salary ~ 1 + YearsExperience\n",
       "\n",
       "Coefficients:\n",
       "────────────────────────────────────────────────────────────────────────────\n",
       "                    Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "────────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)      25792.2     2273.05   11.35    <1e-11   21136.1     30448.3\n",
       "YearsExperience   9449.96     378.755  24.95    <1e-19    8674.12    10225.8\n",
       "────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols=lm(@formula(Salary ~ YearsExperience), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 2273.05343\n",
       "  378.75457"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round.(stderror(ols), digits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have p distinct predictors. Then the multiple linear regression model takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Y = β0 +β1X1 +β2X2 +···+βpXp +ε"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where Xj represents the jth predictor and βj quantifies the association between that variable and the response. We interpret βj as the average effect on Y of a one unit increase in Xj, holding all other predictors fixed. \n",
    "\n",
    "In the advertising example the above eqaution becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sales=β0 +β1 ×TV+β2 ×radio+β3 ×newspaper+ε."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose β0, β1, . . . , βp to minimize the sum of squared residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, GLM, RDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Country</th><th>SR</th><th>Pop15</th><th>Pop75</th><th>DPI</th><th>DDPI</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>50 rows × 6 columns</p><tr><th>1</th><td>Australia</td><td>11.43</td><td>29.35</td><td>2.87</td><td>2329.68</td><td>2.87</td></tr><tr><th>2</th><td>Austria</td><td>12.07</td><td>23.32</td><td>4.41</td><td>1507.99</td><td>3.93</td></tr><tr><th>3</th><td>Belgium</td><td>13.17</td><td>23.8</td><td>4.43</td><td>2108.47</td><td>3.82</td></tr><tr><th>4</th><td>Bolivia</td><td>5.75</td><td>41.89</td><td>1.67</td><td>189.13</td><td>0.22</td></tr><tr><th>5</th><td>Brazil</td><td>12.88</td><td>42.19</td><td>0.83</td><td>728.47</td><td>4.56</td></tr><tr><th>6</th><td>Canada</td><td>8.79</td><td>31.72</td><td>2.85</td><td>2982.88</td><td>2.43</td></tr><tr><th>7</th><td>Chile</td><td>0.6</td><td>39.74</td><td>1.34</td><td>662.86</td><td>2.67</td></tr><tr><th>8</th><td>China</td><td>11.9</td><td>44.75</td><td>0.67</td><td>289.52</td><td>6.51</td></tr><tr><th>9</th><td>Colombia</td><td>4.98</td><td>46.64</td><td>1.06</td><td>276.65</td><td>3.08</td></tr><tr><th>10</th><td>Costa Rica</td><td>10.78</td><td>47.64</td><td>1.14</td><td>471.24</td><td>2.8</td></tr><tr><th>11</th><td>Denmark</td><td>16.85</td><td>24.42</td><td>3.93</td><td>2496.53</td><td>3.99</td></tr><tr><th>12</th><td>Ecuador</td><td>3.59</td><td>46.31</td><td>1.19</td><td>287.77</td><td>2.19</td></tr><tr><th>13</th><td>Finland</td><td>11.24</td><td>27.84</td><td>2.37</td><td>1681.25</td><td>4.32</td></tr><tr><th>14</th><td>France</td><td>12.64</td><td>25.06</td><td>4.7</td><td>2213.82</td><td>4.52</td></tr><tr><th>15</th><td>Germany</td><td>12.55</td><td>23.31</td><td>3.35</td><td>2457.12</td><td>3.44</td></tr><tr><th>16</th><td>Greece</td><td>10.67</td><td>25.62</td><td>3.1</td><td>870.85</td><td>6.28</td></tr><tr><th>17</th><td>Guatamala</td><td>3.01</td><td>46.05</td><td>0.87</td><td>289.71</td><td>1.48</td></tr><tr><th>18</th><td>Honduras</td><td>7.7</td><td>47.32</td><td>0.58</td><td>232.44</td><td>3.19</td></tr><tr><th>19</th><td>Iceland</td><td>1.27</td><td>34.03</td><td>3.08</td><td>1900.1</td><td>1.12</td></tr><tr><th>20</th><td>India</td><td>9.0</td><td>41.31</td><td>0.96</td><td>88.94</td><td>1.54</td></tr><tr><th>21</th><td>Ireland</td><td>11.34</td><td>31.16</td><td>4.19</td><td>1139.95</td><td>2.99</td></tr><tr><th>22</th><td>Italy</td><td>14.28</td><td>24.52</td><td>3.48</td><td>1390.0</td><td>3.54</td></tr><tr><th>23</th><td>Japan</td><td>21.1</td><td>27.01</td><td>1.91</td><td>1257.28</td><td>8.21</td></tr><tr><th>24</th><td>Korea</td><td>3.98</td><td>41.74</td><td>0.91</td><td>207.68</td><td>5.81</td></tr><tr><th>25</th><td>Luxembourg</td><td>10.35</td><td>21.8</td><td>3.73</td><td>2449.39</td><td>1.57</td></tr><tr><th>26</th><td>Malta</td><td>15.48</td><td>32.54</td><td>2.47</td><td>601.05</td><td>8.12</td></tr><tr><th>27</th><td>Norway</td><td>10.25</td><td>25.95</td><td>3.67</td><td>2231.03</td><td>3.62</td></tr><tr><th>28</th><td>Netherlands</td><td>14.65</td><td>24.71</td><td>3.25</td><td>1740.7</td><td>7.66</td></tr><tr><th>29</th><td>New Zealand</td><td>10.67</td><td>32.61</td><td>3.17</td><td>1487.52</td><td>1.76</td></tr><tr><th>30</th><td>Nicaragua</td><td>7.3</td><td>45.04</td><td>1.21</td><td>325.54</td><td>2.48</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& Country & SR & Pop15 & Pop75 & DPI & DDPI\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & Australia & 11.43 & 29.35 & 2.87 & 2329.68 & 2.87 \\\\\n",
       "\t2 & Austria & 12.07 & 23.32 & 4.41 & 1507.99 & 3.93 \\\\\n",
       "\t3 & Belgium & 13.17 & 23.8 & 4.43 & 2108.47 & 3.82 \\\\\n",
       "\t4 & Bolivia & 5.75 & 41.89 & 1.67 & 189.13 & 0.22 \\\\\n",
       "\t5 & Brazil & 12.88 & 42.19 & 0.83 & 728.47 & 4.56 \\\\\n",
       "\t6 & Canada & 8.79 & 31.72 & 2.85 & 2982.88 & 2.43 \\\\\n",
       "\t7 & Chile & 0.6 & 39.74 & 1.34 & 662.86 & 2.67 \\\\\n",
       "\t8 & China & 11.9 & 44.75 & 0.67 & 289.52 & 6.51 \\\\\n",
       "\t9 & Colombia & 4.98 & 46.64 & 1.06 & 276.65 & 3.08 \\\\\n",
       "\t10 & Costa Rica & 10.78 & 47.64 & 1.14 & 471.24 & 2.8 \\\\\n",
       "\t11 & Denmark & 16.85 & 24.42 & 3.93 & 2496.53 & 3.99 \\\\\n",
       "\t12 & Ecuador & 3.59 & 46.31 & 1.19 & 287.77 & 2.19 \\\\\n",
       "\t13 & Finland & 11.24 & 27.84 & 2.37 & 1681.25 & 4.32 \\\\\n",
       "\t14 & France & 12.64 & 25.06 & 4.7 & 2213.82 & 4.52 \\\\\n",
       "\t15 & Germany & 12.55 & 23.31 & 3.35 & 2457.12 & 3.44 \\\\\n",
       "\t16 & Greece & 10.67 & 25.62 & 3.1 & 870.85 & 6.28 \\\\\n",
       "\t17 & Guatamala & 3.01 & 46.05 & 0.87 & 289.71 & 1.48 \\\\\n",
       "\t18 & Honduras & 7.7 & 47.32 & 0.58 & 232.44 & 3.19 \\\\\n",
       "\t19 & Iceland & 1.27 & 34.03 & 3.08 & 1900.1 & 1.12 \\\\\n",
       "\t20 & India & 9.0 & 41.31 & 0.96 & 88.94 & 1.54 \\\\\n",
       "\t21 & Ireland & 11.34 & 31.16 & 4.19 & 1139.95 & 2.99 \\\\\n",
       "\t22 & Italy & 14.28 & 24.52 & 3.48 & 1390.0 & 3.54 \\\\\n",
       "\t23 & Japan & 21.1 & 27.01 & 1.91 & 1257.28 & 8.21 \\\\\n",
       "\t24 & Korea & 3.98 & 41.74 & 0.91 & 207.68 & 5.81 \\\\\n",
       "\t25 & Luxembourg & 10.35 & 21.8 & 3.73 & 2449.39 & 1.57 \\\\\n",
       "\t26 & Malta & 15.48 & 32.54 & 2.47 & 601.05 & 8.12 \\\\\n",
       "\t27 & Norway & 10.25 & 25.95 & 3.67 & 2231.03 & 3.62 \\\\\n",
       "\t28 & Netherlands & 14.65 & 24.71 & 3.25 & 1740.7 & 7.66 \\\\\n",
       "\t29 & New Zealand & 10.67 & 32.61 & 3.17 & 1487.52 & 1.76 \\\\\n",
       "\t30 & Nicaragua & 7.3 & 45.04 & 1.21 & 325.54 & 2.48 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "50×6 DataFrame\n",
       "│ Row │ Country        │ SR      │ Pop15   │ Pop75   │ DPI     │ DDPI    │\n",
       "│     │ \u001b[90mString\u001b[39m         │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ Australia      │ 11.43   │ 29.35   │ 2.87    │ 2329.68 │ 2.87    │\n",
       "│ 2   │ Austria        │ 12.07   │ 23.32   │ 4.41    │ 1507.99 │ 3.93    │\n",
       "│ 3   │ Belgium        │ 13.17   │ 23.8    │ 4.43    │ 2108.47 │ 3.82    │\n",
       "│ 4   │ Bolivia        │ 5.75    │ 41.89   │ 1.67    │ 189.13  │ 0.22    │\n",
       "│ 5   │ Brazil         │ 12.88   │ 42.19   │ 0.83    │ 728.47  │ 4.56    │\n",
       "│ 6   │ Canada         │ 8.79    │ 31.72   │ 2.85    │ 2982.88 │ 2.43    │\n",
       "│ 7   │ Chile          │ 0.6     │ 39.74   │ 1.34    │ 662.86  │ 2.67    │\n",
       "│ 8   │ China          │ 11.9    │ 44.75   │ 0.67    │ 289.52  │ 6.51    │\n",
       "│ 9   │ Colombia       │ 4.98    │ 46.64   │ 1.06    │ 276.65  │ 3.08    │\n",
       "│ 10  │ Costa Rica     │ 10.78   │ 47.64   │ 1.14    │ 471.24  │ 2.8     │\n",
       "⋮\n",
       "│ 40  │ Switzerland    │ 14.13   │ 23.49   │ 3.73    │ 2630.96 │ 2.7     │\n",
       "│ 41  │ Turkey         │ 5.13    │ 43.42   │ 1.08    │ 389.66  │ 2.96    │\n",
       "│ 42  │ Tunisia        │ 2.81    │ 46.12   │ 1.21    │ 249.87  │ 1.13    │\n",
       "│ 43  │ United Kingdom │ 7.81    │ 23.27   │ 4.46    │ 1813.93 │ 2.01    │\n",
       "│ 44  │ United States  │ 7.56    │ 29.81   │ 3.43    │ 4001.89 │ 2.45    │\n",
       "│ 45  │ Venezuela      │ 9.22    │ 46.4    │ 0.9     │ 813.39  │ 0.53    │\n",
       "│ 46  │ Zambia         │ 18.56   │ 45.25   │ 0.56    │ 138.33  │ 5.14    │\n",
       "│ 47  │ Jamaica        │ 7.72    │ 41.12   │ 1.73    │ 380.47  │ 10.23   │\n",
       "│ 48  │ Uruguay        │ 9.24    │ 28.13   │ 2.72    │ 766.54  │ 1.88    │\n",
       "│ 49  │ Libya          │ 8.89    │ 43.69   │ 2.07    │ 123.58  │ 16.71   │\n",
       "│ 50  │ Malaysia       │ 4.71    │ 47.2    │ 0.66    │ 242.69  │ 5.08    │"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LifeCycleSavings = dataset(\"datasets\", \"LifeCycleSavings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "SR ~ 1 + Pop15 + Pop75 + DPI + DDPI\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────────────────────────\n",
       "                    Coef.   Std. Error      t  Pr(>|t|)    Lower 95%    Upper 95%\n",
       "─────────────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  28.5661       7.35452       3.88    0.0003  13.7533      43.3788\n",
       "Pop15        -0.461193     0.144642     -3.19    0.0026  -0.752518    -0.169869\n",
       "Pop75        -1.6915       1.0836       -1.56    0.1255  -3.87398      0.490983\n",
       "DPI          -0.000336902  0.000931107  -0.36    0.7192  -0.00221225   0.00153844\n",
       "DDPI          0.409695     0.196197      2.09    0.0425   0.0145336    0.804856\n",
       "─────────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm2 = fit(LinearModel, @formula(SR ~ Pop15 + Pop75 + DPI + DDPI), LifeCycleSavings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Important Questions\n",
    "\n",
    "When we perform multiple linear regression, we usually are interested in answering a few important questions.\n",
    "\n",
    "1. Is at least one of the predictors X1 , X2 , . . . , Xp useful in predicting the response?\n",
    "2. Do all the predictors help to explain Y, or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict,and how accurate is our prediction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Considerations in the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Qualitative Predictors\n",
    "\n",
    "In our discussion so far, we have assumed that all variables in our linear regression model are quantitative. But in practice, this is not necessarily the case; often some predictors are qualitative.\n",
    "\n",
    "### Predictors with Only Two Levels\n",
    "\n",
    "Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a\n",
    "qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We\n",
    "factor level\n",
    "simply create an indicator or dummy variable that takes on two possible\n",
    "dummy\n",
    "numerical values.\n",
    "\n",
    "### Qualitative Predictors with More than Two Levels\n",
    "\n",
    "When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Eth</th><th>Sex</th><th>Age</th><th>Lrn</th><th>Days</th></tr><tr><th></th><th>Cat…</th><th>Cat…</th><th>Cat…</th><th>Cat…</th><th>Int32</th></tr></thead><tbody><p>146 rows × 5 columns</p><tr><th>1</th><td>A</td><td>M</td><td>F0</td><td>SL</td><td>2</td></tr><tr><th>2</th><td>A</td><td>M</td><td>F0</td><td>SL</td><td>11</td></tr><tr><th>3</th><td>A</td><td>M</td><td>F0</td><td>SL</td><td>14</td></tr><tr><th>4</th><td>A</td><td>M</td><td>F0</td><td>AL</td><td>5</td></tr><tr><th>5</th><td>A</td><td>M</td><td>F0</td><td>AL</td><td>5</td></tr><tr><th>6</th><td>A</td><td>M</td><td>F0</td><td>AL</td><td>13</td></tr><tr><th>7</th><td>A</td><td>M</td><td>F0</td><td>AL</td><td>20</td></tr><tr><th>8</th><td>A</td><td>M</td><td>F0</td><td>AL</td><td>22</td></tr><tr><th>9</th><td>A</td><td>M</td><td>F1</td><td>SL</td><td>6</td></tr><tr><th>10</th><td>A</td><td>M</td><td>F1</td><td>SL</td><td>6</td></tr><tr><th>11</th><td>A</td><td>M</td><td>F1</td><td>SL</td><td>15</td></tr><tr><th>12</th><td>A</td><td>M</td><td>F1</td><td>AL</td><td>7</td></tr><tr><th>13</th><td>A</td><td>M</td><td>F1</td><td>AL</td><td>14</td></tr><tr><th>14</th><td>A</td><td>M</td><td>F2</td><td>SL</td><td>6</td></tr><tr><th>15</th><td>A</td><td>M</td><td>F2</td><td>SL</td><td>32</td></tr><tr><th>16</th><td>A</td><td>M</td><td>F2</td><td>SL</td><td>53</td></tr><tr><th>17</th><td>A</td><td>M</td><td>F2</td><td>SL</td><td>57</td></tr><tr><th>18</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>14</td></tr><tr><th>19</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>16</td></tr><tr><th>20</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>16</td></tr><tr><th>21</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>17</td></tr><tr><th>22</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>40</td></tr><tr><th>23</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>43</td></tr><tr><th>24</th><td>A</td><td>M</td><td>F2</td><td>AL</td><td>46</td></tr><tr><th>25</th><td>A</td><td>M</td><td>F3</td><td>AL</td><td>8</td></tr><tr><th>26</th><td>A</td><td>M</td><td>F3</td><td>AL</td><td>23</td></tr><tr><th>27</th><td>A</td><td>M</td><td>F3</td><td>AL</td><td>23</td></tr><tr><th>28</th><td>A</td><td>M</td><td>F3</td><td>AL</td><td>28</td></tr><tr><th>29</th><td>A</td><td>M</td><td>F3</td><td>AL</td><td>34</td></tr><tr><th>30</th><td>A</td><td>M</td><td>F3</td><td>AL</td><td>36</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Eth & Sex & Age & Lrn & Days\\\\\n",
       "\t\\hline\n",
       "\t& Cat… & Cat… & Cat… & Cat… & Int32\\\\\n",
       "\t\\hline\n",
       "\t1 & A & M & F0 & SL & 2 \\\\\n",
       "\t2 & A & M & F0 & SL & 11 \\\\\n",
       "\t3 & A & M & F0 & SL & 14 \\\\\n",
       "\t4 & A & M & F0 & AL & 5 \\\\\n",
       "\t5 & A & M & F0 & AL & 5 \\\\\n",
       "\t6 & A & M & F0 & AL & 13 \\\\\n",
       "\t7 & A & M & F0 & AL & 20 \\\\\n",
       "\t8 & A & M & F0 & AL & 22 \\\\\n",
       "\t9 & A & M & F1 & SL & 6 \\\\\n",
       "\t10 & A & M & F1 & SL & 6 \\\\\n",
       "\t11 & A & M & F1 & SL & 15 \\\\\n",
       "\t12 & A & M & F1 & AL & 7 \\\\\n",
       "\t13 & A & M & F1 & AL & 14 \\\\\n",
       "\t14 & A & M & F2 & SL & 6 \\\\\n",
       "\t15 & A & M & F2 & SL & 32 \\\\\n",
       "\t16 & A & M & F2 & SL & 53 \\\\\n",
       "\t17 & A & M & F2 & SL & 57 \\\\\n",
       "\t18 & A & M & F2 & AL & 14 \\\\\n",
       "\t19 & A & M & F2 & AL & 16 \\\\\n",
       "\t20 & A & M & F2 & AL & 16 \\\\\n",
       "\t21 & A & M & F2 & AL & 17 \\\\\n",
       "\t22 & A & M & F2 & AL & 40 \\\\\n",
       "\t23 & A & M & F2 & AL & 43 \\\\\n",
       "\t24 & A & M & F2 & AL & 46 \\\\\n",
       "\t25 & A & M & F3 & AL & 8 \\\\\n",
       "\t26 & A & M & F3 & AL & 23 \\\\\n",
       "\t27 & A & M & F3 & AL & 23 \\\\\n",
       "\t28 & A & M & F3 & AL & 28 \\\\\n",
       "\t29 & A & M & F3 & AL & 34 \\\\\n",
       "\t30 & A & M & F3 & AL & 36 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "146×5 DataFrame\n",
       "│ Row │ Eth  │ Sex  │ Age  │ Lrn  │ Days  │\n",
       "│     │ \u001b[90mCat…\u001b[39m │ \u001b[90mCat…\u001b[39m │ \u001b[90mCat…\u001b[39m │ \u001b[90mCat…\u001b[39m │ \u001b[90mInt32\u001b[39m │\n",
       "├─────┼──────┼──────┼──────┼──────┼───────┤\n",
       "│ 1   │ A    │ M    │ F0   │ SL   │ 2     │\n",
       "│ 2   │ A    │ M    │ F0   │ SL   │ 11    │\n",
       "│ 3   │ A    │ M    │ F0   │ SL   │ 14    │\n",
       "│ 4   │ A    │ M    │ F0   │ AL   │ 5     │\n",
       "│ 5   │ A    │ M    │ F0   │ AL   │ 5     │\n",
       "│ 6   │ A    │ M    │ F0   │ AL   │ 13    │\n",
       "│ 7   │ A    │ M    │ F0   │ AL   │ 20    │\n",
       "│ 8   │ A    │ M    │ F0   │ AL   │ 22    │\n",
       "│ 9   │ A    │ M    │ F1   │ SL   │ 6     │\n",
       "│ 10  │ A    │ M    │ F1   │ SL   │ 6     │\n",
       "⋮\n",
       "│ 136 │ N    │ F    │ F2   │ AL   │ 1     │\n",
       "│ 137 │ N    │ F    │ F3   │ AL   │ 1     │\n",
       "│ 138 │ N    │ F    │ F3   │ AL   │ 9     │\n",
       "│ 139 │ N    │ F    │ F3   │ AL   │ 22    │\n",
       "│ 140 │ N    │ F    │ F3   │ AL   │ 3     │\n",
       "│ 141 │ N    │ F    │ F3   │ AL   │ 3     │\n",
       "│ 142 │ N    │ F    │ F3   │ AL   │ 5     │\n",
       "│ 143 │ N    │ F    │ F3   │ AL   │ 15    │\n",
       "│ 144 │ N    │ F    │ F3   │ AL   │ 18    │\n",
       "│ 145 │ N    │ F    │ F3   │ AL   │ 22    │\n",
       "│ 146 │ N    │ F    │ F3   │ AL   │ 37    │"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quine = dataset(\"MASS\", \"quine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},NegativeBinomial{Float64},LogLink},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "Days ~ 1 + Eth + Sex + Age + Lrn\n",
       "\n",
       "Coefficients:\n",
       "────────────────────────────────────────────────────────────────────────────\n",
       "                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n",
       "────────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)   2.88645      0.227144  12.71    <1e-36   2.44125     3.33164\n",
       "Eth: N       -0.567515     0.152449  -3.72    0.0002  -0.86631    -0.26872\n",
       "Sex: M        0.0870771    0.159025   0.55    0.5840  -0.224606    0.398761\n",
       "Age: F1      -0.445076     0.239087  -1.86    0.0627  -0.913678    0.0235251\n",
       "Age: F2       0.0927999    0.234502   0.40    0.6923  -0.366816    0.552416\n",
       "Age: F3       0.359485     0.246586   1.46    0.1449  -0.123814    0.842784\n",
       "Lrn: SL       0.296768     0.185934   1.60    0.1105  -0.0676559   0.661191\n",
       "────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrmodel = glm(@formula(Days ~ Eth+Sex+Age+Lrn), quine, NegativeBinomial(2.0), LogLink())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},NegativeBinomial{Float64},LogLink},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "Days ~ 1 + Eth + Sex + Age + Lrn\n",
       "\n",
       "Coefficients:\n",
       "────────────────────────────────────────────────────────────────────────────\n",
       "                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n",
       "────────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)   2.89453      0.227415  12.73    <1e-36   2.4488      3.34025\n",
       "Eth: N       -0.569341     0.152656  -3.73    0.0002  -0.868541   -0.270141\n",
       "Sex: M        0.0823881    0.159209   0.52    0.6048  -0.229655    0.394431\n",
       "Age: F1      -0.448464     0.238687  -1.88    0.0603  -0.916281    0.0193536\n",
       "Age: F2       0.0880506    0.235149   0.37    0.7081  -0.372834    0.548935\n",
       "Age: F3       0.356955     0.247228   1.44    0.1488  -0.127602    0.841513\n",
       "Lrn: SL       0.292138     0.18565    1.57    0.1156  -0.0717297   0.656006\n",
       "────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrmodel = negbin(@formula(Days ~ Eth+Sex+Age+Lrn), quine, LogLink())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated theta = 1.27489\n"
     ]
    }
   ],
   "source": [
    "println(\"Estimated theta = \", round(nbrmodel.model.rr.d.r, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions of the Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The standard linear regression model provides interpretable results and works quite well on many real-world problems. However, it makes several highly restrictive assumptions that are often violated in practice. Two of the most important assumptions state that the relationship between the predictors and response are additive and linear. The additive assumption\n",
    "baseline means that the effect of changes in a predictor Xj on the response Y is independent of the values of the other predictors. The linear assumption states that the change in the response Y due to a one-unit change in Xj is constant, regardless of the value of Xj.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the Additive Assumption\n",
    "\n",
    "Consider the standard linear regression model with two variables,\n",
    "\n",
    "    Y = β0 + β1X1 + β2X2 + ε.\n",
    "\n",
    "According to this model, if we increase X1 by one unit, then Y will increase by an average of β1 units. Notice that the presence of X2 does not alter this statement—that is, regardless of the value of X2, a one-unit increase in X1 will lead to a β1-unit increase in Y . One way of extending this model to allow for interaction effects is to include a third predictor, called an interaction term, which is constructed by computing the product of X1 and X2. This results in the model\n",
    "  \n",
    "    Y = β0 +β1X1 +β2X2 +β3X1X2 +ε.\n",
    "    \n",
    "How does inclusion of this interaction term relax the additive assumption?\n",
    "\n",
    "Notice that (3.31) can be rewritten as\n",
    "\n",
    "    Y = β0 +(β1 +β3X2)X1 +β2X2 +ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Country</th><th>SR</th><th>Pop15</th><th>Pop75</th><th>DPI</th><th>DDPI</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>50 rows × 6 columns</p><tr><th>1</th><td>Australia</td><td>11.43</td><td>29.35</td><td>2.87</td><td>2329.68</td><td>2.87</td></tr><tr><th>2</th><td>Austria</td><td>12.07</td><td>23.32</td><td>4.41</td><td>1507.99</td><td>3.93</td></tr><tr><th>3</th><td>Belgium</td><td>13.17</td><td>23.8</td><td>4.43</td><td>2108.47</td><td>3.82</td></tr><tr><th>4</th><td>Bolivia</td><td>5.75</td><td>41.89</td><td>1.67</td><td>189.13</td><td>0.22</td></tr><tr><th>5</th><td>Brazil</td><td>12.88</td><td>42.19</td><td>0.83</td><td>728.47</td><td>4.56</td></tr><tr><th>6</th><td>Canada</td><td>8.79</td><td>31.72</td><td>2.85</td><td>2982.88</td><td>2.43</td></tr><tr><th>7</th><td>Chile</td><td>0.6</td><td>39.74</td><td>1.34</td><td>662.86</td><td>2.67</td></tr><tr><th>8</th><td>China</td><td>11.9</td><td>44.75</td><td>0.67</td><td>289.52</td><td>6.51</td></tr><tr><th>9</th><td>Colombia</td><td>4.98</td><td>46.64</td><td>1.06</td><td>276.65</td><td>3.08</td></tr><tr><th>10</th><td>Costa Rica</td><td>10.78</td><td>47.64</td><td>1.14</td><td>471.24</td><td>2.8</td></tr><tr><th>11</th><td>Denmark</td><td>16.85</td><td>24.42</td><td>3.93</td><td>2496.53</td><td>3.99</td></tr><tr><th>12</th><td>Ecuador</td><td>3.59</td><td>46.31</td><td>1.19</td><td>287.77</td><td>2.19</td></tr><tr><th>13</th><td>Finland</td><td>11.24</td><td>27.84</td><td>2.37</td><td>1681.25</td><td>4.32</td></tr><tr><th>14</th><td>France</td><td>12.64</td><td>25.06</td><td>4.7</td><td>2213.82</td><td>4.52</td></tr><tr><th>15</th><td>Germany</td><td>12.55</td><td>23.31</td><td>3.35</td><td>2457.12</td><td>3.44</td></tr><tr><th>16</th><td>Greece</td><td>10.67</td><td>25.62</td><td>3.1</td><td>870.85</td><td>6.28</td></tr><tr><th>17</th><td>Guatamala</td><td>3.01</td><td>46.05</td><td>0.87</td><td>289.71</td><td>1.48</td></tr><tr><th>18</th><td>Honduras</td><td>7.7</td><td>47.32</td><td>0.58</td><td>232.44</td><td>3.19</td></tr><tr><th>19</th><td>Iceland</td><td>1.27</td><td>34.03</td><td>3.08</td><td>1900.1</td><td>1.12</td></tr><tr><th>20</th><td>India</td><td>9.0</td><td>41.31</td><td>0.96</td><td>88.94</td><td>1.54</td></tr><tr><th>21</th><td>Ireland</td><td>11.34</td><td>31.16</td><td>4.19</td><td>1139.95</td><td>2.99</td></tr><tr><th>22</th><td>Italy</td><td>14.28</td><td>24.52</td><td>3.48</td><td>1390.0</td><td>3.54</td></tr><tr><th>23</th><td>Japan</td><td>21.1</td><td>27.01</td><td>1.91</td><td>1257.28</td><td>8.21</td></tr><tr><th>24</th><td>Korea</td><td>3.98</td><td>41.74</td><td>0.91</td><td>207.68</td><td>5.81</td></tr><tr><th>25</th><td>Luxembourg</td><td>10.35</td><td>21.8</td><td>3.73</td><td>2449.39</td><td>1.57</td></tr><tr><th>26</th><td>Malta</td><td>15.48</td><td>32.54</td><td>2.47</td><td>601.05</td><td>8.12</td></tr><tr><th>27</th><td>Norway</td><td>10.25</td><td>25.95</td><td>3.67</td><td>2231.03</td><td>3.62</td></tr><tr><th>28</th><td>Netherlands</td><td>14.65</td><td>24.71</td><td>3.25</td><td>1740.7</td><td>7.66</td></tr><tr><th>29</th><td>New Zealand</td><td>10.67</td><td>32.61</td><td>3.17</td><td>1487.52</td><td>1.76</td></tr><tr><th>30</th><td>Nicaragua</td><td>7.3</td><td>45.04</td><td>1.21</td><td>325.54</td><td>2.48</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& Country & SR & Pop15 & Pop75 & DPI & DDPI\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & Australia & 11.43 & 29.35 & 2.87 & 2329.68 & 2.87 \\\\\n",
       "\t2 & Austria & 12.07 & 23.32 & 4.41 & 1507.99 & 3.93 \\\\\n",
       "\t3 & Belgium & 13.17 & 23.8 & 4.43 & 2108.47 & 3.82 \\\\\n",
       "\t4 & Bolivia & 5.75 & 41.89 & 1.67 & 189.13 & 0.22 \\\\\n",
       "\t5 & Brazil & 12.88 & 42.19 & 0.83 & 728.47 & 4.56 \\\\\n",
       "\t6 & Canada & 8.79 & 31.72 & 2.85 & 2982.88 & 2.43 \\\\\n",
       "\t7 & Chile & 0.6 & 39.74 & 1.34 & 662.86 & 2.67 \\\\\n",
       "\t8 & China & 11.9 & 44.75 & 0.67 & 289.52 & 6.51 \\\\\n",
       "\t9 & Colombia & 4.98 & 46.64 & 1.06 & 276.65 & 3.08 \\\\\n",
       "\t10 & Costa Rica & 10.78 & 47.64 & 1.14 & 471.24 & 2.8 \\\\\n",
       "\t11 & Denmark & 16.85 & 24.42 & 3.93 & 2496.53 & 3.99 \\\\\n",
       "\t12 & Ecuador & 3.59 & 46.31 & 1.19 & 287.77 & 2.19 \\\\\n",
       "\t13 & Finland & 11.24 & 27.84 & 2.37 & 1681.25 & 4.32 \\\\\n",
       "\t14 & France & 12.64 & 25.06 & 4.7 & 2213.82 & 4.52 \\\\\n",
       "\t15 & Germany & 12.55 & 23.31 & 3.35 & 2457.12 & 3.44 \\\\\n",
       "\t16 & Greece & 10.67 & 25.62 & 3.1 & 870.85 & 6.28 \\\\\n",
       "\t17 & Guatamala & 3.01 & 46.05 & 0.87 & 289.71 & 1.48 \\\\\n",
       "\t18 & Honduras & 7.7 & 47.32 & 0.58 & 232.44 & 3.19 \\\\\n",
       "\t19 & Iceland & 1.27 & 34.03 & 3.08 & 1900.1 & 1.12 \\\\\n",
       "\t20 & India & 9.0 & 41.31 & 0.96 & 88.94 & 1.54 \\\\\n",
       "\t21 & Ireland & 11.34 & 31.16 & 4.19 & 1139.95 & 2.99 \\\\\n",
       "\t22 & Italy & 14.28 & 24.52 & 3.48 & 1390.0 & 3.54 \\\\\n",
       "\t23 & Japan & 21.1 & 27.01 & 1.91 & 1257.28 & 8.21 \\\\\n",
       "\t24 & Korea & 3.98 & 41.74 & 0.91 & 207.68 & 5.81 \\\\\n",
       "\t25 & Luxembourg & 10.35 & 21.8 & 3.73 & 2449.39 & 1.57 \\\\\n",
       "\t26 & Malta & 15.48 & 32.54 & 2.47 & 601.05 & 8.12 \\\\\n",
       "\t27 & Norway & 10.25 & 25.95 & 3.67 & 2231.03 & 3.62 \\\\\n",
       "\t28 & Netherlands & 14.65 & 24.71 & 3.25 & 1740.7 & 7.66 \\\\\n",
       "\t29 & New Zealand & 10.67 & 32.61 & 3.17 & 1487.52 & 1.76 \\\\\n",
       "\t30 & Nicaragua & 7.3 & 45.04 & 1.21 & 325.54 & 2.48 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "50×6 DataFrame\n",
       "│ Row │ Country        │ SR      │ Pop15   │ Pop75   │ DPI     │ DDPI    │\n",
       "│     │ \u001b[90mString\u001b[39m         │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ Australia      │ 11.43   │ 29.35   │ 2.87    │ 2329.68 │ 2.87    │\n",
       "│ 2   │ Austria        │ 12.07   │ 23.32   │ 4.41    │ 1507.99 │ 3.93    │\n",
       "│ 3   │ Belgium        │ 13.17   │ 23.8    │ 4.43    │ 2108.47 │ 3.82    │\n",
       "│ 4   │ Bolivia        │ 5.75    │ 41.89   │ 1.67    │ 189.13  │ 0.22    │\n",
       "│ 5   │ Brazil         │ 12.88   │ 42.19   │ 0.83    │ 728.47  │ 4.56    │\n",
       "│ 6   │ Canada         │ 8.79    │ 31.72   │ 2.85    │ 2982.88 │ 2.43    │\n",
       "│ 7   │ Chile          │ 0.6     │ 39.74   │ 1.34    │ 662.86  │ 2.67    │\n",
       "│ 8   │ China          │ 11.9    │ 44.75   │ 0.67    │ 289.52  │ 6.51    │\n",
       "│ 9   │ Colombia       │ 4.98    │ 46.64   │ 1.06    │ 276.65  │ 3.08    │\n",
       "│ 10  │ Costa Rica     │ 10.78   │ 47.64   │ 1.14    │ 471.24  │ 2.8     │\n",
       "⋮\n",
       "│ 40  │ Switzerland    │ 14.13   │ 23.49   │ 3.73    │ 2630.96 │ 2.7     │\n",
       "│ 41  │ Turkey         │ 5.13    │ 43.42   │ 1.08    │ 389.66  │ 2.96    │\n",
       "│ 42  │ Tunisia        │ 2.81    │ 46.12   │ 1.21    │ 249.87  │ 1.13    │\n",
       "│ 43  │ United Kingdom │ 7.81    │ 23.27   │ 4.46    │ 1813.93 │ 2.01    │\n",
       "│ 44  │ United States  │ 7.56    │ 29.81   │ 3.43    │ 4001.89 │ 2.45    │\n",
       "│ 45  │ Venezuela      │ 9.22    │ 46.4    │ 0.9     │ 813.39  │ 0.53    │\n",
       "│ 46  │ Zambia         │ 18.56   │ 45.25   │ 0.56    │ 138.33  │ 5.14    │\n",
       "│ 47  │ Jamaica        │ 7.72    │ 41.12   │ 1.73    │ 380.47  │ 10.23   │\n",
       "│ 48  │ Uruguay        │ 9.24    │ 28.13   │ 2.72    │ 766.54  │ 1.88    │\n",
       "│ 49  │ Libya          │ 8.89    │ 43.69   │ 2.07    │ 123.58  │ 16.71   │\n",
       "│ 50  │ Malaysia       │ 4.71    │ 47.2    │ 0.66    │ 242.69  │ 5.08    │"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LifeCycleSavings = dataset(\"datasets\", \"LifeCycleSavings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "SR ~ 1 + Pop15 + Pop75 + Pop15 & Pop75\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────────────────────\n",
       "                    Coef.  Std. Error      t  Pr(>|t|)  Lower 95%   Upper 95%\n",
       "─────────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)    29.2303      8.54153     3.42    0.0013  12.0371    46.4236\n",
       "Pop15          -0.428707    0.193501   -2.22    0.0317  -0.818204  -0.0392103\n",
       "Pop75          -1.16446     2.50349    -0.47    0.6440  -6.20373    3.8748\n",
       "Pop15 & Pop75  -0.0263937   0.0779213  -0.34    0.7364  -0.183241   0.130454\n",
       "─────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm2 = fit(LinearModel, @formula(SR ~ Pop15 + Pop75 +Pop15*Pop75), LifeCycleSavings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear Relationships\n",
    "\n",
    "As discussed previously, the linear regression model  assumes a linear relationship between the response and predictors. But in some cases, the true relationship between the response and the predictors may be non- linear. Here we present a very simple way to directly extend the linear model to accommodate non-linear relationships, using polynomial regression. In later chapters, we will present more complex approaches for performing non-linear fits in more general settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "y ~ 1 + x + :(x ^ 2)\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "                    Coef.   Std. Error                    t  Pr(>|t|)     Lower 95%     Upper 95%\n",
       "─────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)   5.31486e-13  6.82708e-14                 7.78    <1e-11   3.95987e-13   6.66985e-13\n",
       "x            -0.5          3.12016e-15  -160248115641061.69    <1e-99  -0.5          -0.5\n",
       "x ^ 2         0.1          2.99305e-17  3341071373231469.50    <1e-99   0.1           0.1\n",
       "─────────────────────────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1*x^2 - 0.5*x for x ∈ 1:100]\n",
    "x = collect(1:100)\n",
    "df = (y = y, x = x)\n",
    "\n",
    "ols=fit(LinearModel, @formula(y ~ x + x^2), df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit a linear regression model to a particular data set, many prob- lems may occur. Most common among these are the following:\n",
    "1. Non-linearity of the response-predictor relationships. 2. Correlation of error terms.\n",
    "3. Non-constant variance of error terms.\n",
    "4. Outliers.\n",
    "5. High-leverage points.\n",
    "6. Collinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building a number of different regression models, there is a wealth of criteria by which they can be evaluated and compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error\n",
    "\n",
    "RMSE is a popular formula to measure the error rate of a regression model. However, it can only be compared between models whose errors are measured in the same units.\t\t\n",
    "<img src=\"RMSE.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Squared Error\n",
    "\n",
    "Unlike RMSE, the relative squared error (RSE) can be compared between models whose errors are measured in the different units.\n",
    "<img src=\"Rela.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Absolute Error\n",
    "\n",
    "Like RSE , the relative absolute error (RAE) can be compared between models whose errors are measured in the different units.\n",
    "<img src=\"RAE.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination\n",
    "\n",
    "The coefficient of determination (R2) summarizes the explanatory power of the regression model and is computed from the sums-of-squares terms.\n",
    "\n",
    "<img src=\"COD.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "R2 describes the proportion of variance of the dependent variable explained by the regression model. If the regression model is “perfect”, SSE is zero, and R2 is 1. If the regression model is a total failure, SSE is equal to SST, no variance is explained by regression, and R2 is zero.\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often qualitative variables are referred to as categorical; we will use these terms interchangeably. In this chapter, we study approaches for predicting qualitative responses, a process that is known as classification. Predicting a qualitative response for an obser- vation can be referred to as classifying that observation, since it involves assigning the observation to a category, or class.  On the other hand, often the methods used for classification first predict the probability of each of the categories of a qualitative variable, as the basis for making the classi- fication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we discuss three of the most\n",
    "qualitative\n",
    "classification\n",
    "classifier\n",
    "widely-used classifiers: logistic regression, linear discriminant analysis, and\n",
    "logistic\n",
    "K-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification setting we have a set of training observations (x1,y1),...,(xn,yn) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Not Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses: stroke, drug overdose, and epileptic seizure. We could consider encoding these values as a quantitative response variable,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares could be used to fit a linear regression model to predict Y on the basis of a set of predictors X1 , . . . , Xp . Unfortunately, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure, and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we use linear regression, some of our estimates might be outside the [0, 1] interval (see Figure), making them hard to interpret as probabilities! Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure \n",
    "\n",
    "<img src=\"linear.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the dummy variable approach cannot be easily extended to accommodate qualitative responses with more than two levels. For these reasons, it is preferable to use a classification method that is truly suited for qualitative response values, such as the ones presented next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the Default data set, where the response default falls into one of two categories, Yes or No. Rather than modeling this response Y directly, logistic regression models the probability that Y belongs to a particular category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as\n",
    "     \n",
    "     Pr(default = Yes|balance).\n",
    "The values of Pr(default = Yes|balance), which we abbreviate p(balance), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default.\n",
    "\n",
    "<img src=\"logisticc.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Model\n",
    "How should we model the relationship between p(X) = Pr(Y = 1|X) and X? (For convenience we are using the generic 0/1 coding for the response)\n",
    "\n",
    "    p(X) = β0 + β1X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach: for balances close to zero we predict a negative probability of default; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1. This problem is not unique to the credit default data. Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict p(X) < 0 for some values of X and p(X) > 1 for others (unless the range of X is limited).\n",
    "\n",
    "To avoid this problem, we must model p(X) using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the logistic function,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "𝑝(𝑋)=\\frac{𝑒^{β0+β1𝑋}}{1+𝑒^{β0+β1𝑋}}\n",
    " \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model, we use a method called maximum likelihood, low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X, we will obtain a sensible prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "\\frac{𝑝(𝑋)}{1 − p(X)}=𝑒^{β0+β1𝑋}\n",
    " \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity p(X)/[1−p(X)] is called the odds, and can take on any value between 0 and ∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the logarithm of both sides of, we arrive at\n",
    "\\begin{equation*}\n",
    "log(\\frac{𝑝(𝑋)}{1 − p(X)})={β0+β1𝑋}\n",
    " \\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left-hand side is called the log-odds or logit. We see that the logistic regression model has a logit that is linear in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the least squares approach to estimate the unknown linear regression coefficients. Although we could use (non-linear) least squares to fit the linear model, the more general method of maximum likelihood is preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for β0 and β1 such that the predicted probability pˆ(xi) of default for each individual, corresponds as closely as possible to the individual’s observed default status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `getindex(df::DataFrame, col_inds::Union{AbstractVector, Regex, Not})` is deprecated, use `df[:, col_inds]` instead.\n",
      "│   caller = top-level scope at In[42]:4\n",
      "└ @ Core In[42]:4\n",
      "┌ Warning: `getindex(df::DataFrame, col_ind::ColumnIndex)` is deprecated, use `df[!, col_ind]` instead.\n",
      "│   caller = top-level scope at In[42]:8\n",
      "└ @ Core In[42]:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150-element Array{String,1}:\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " ⋮\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RDatasets: dataset\n",
    "\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "\n",
    "# ScikitLearn.jl expects arrays, but DataFrames can also be used - see\n",
    "# the corresponding section of the manual\n",
    "X = convert(Array, iris[[:SepalLength]])\n",
    "y = convert(Array, iris[:Species])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "# This model requires scikit-learn. See\n",
    "# http://scikitlearnjl.readthedocs.io/en/latest/models/#installation\n",
    "@sk_import linear_model: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LogisticRegression()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "fit!(model, X, y)\n",
    "\n",
    "accuracy = sum(predict(model, X) .== y) / length(y)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression, we can generalize logistic regression as follows:\n",
    "􏰃􏰄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "log(\\frac{𝑝(𝑋)}{1 − p(X)})={β0+β1𝑋+β2𝑋2+β3𝑋3+β4𝑋4....βp𝑋p}\n",
    " \\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where X = (X1, . . . , Xp) are p predictors\n",
    "\n",
    "\\begin{equation*}\n",
    "𝑝(𝑋)=\\frac{𝑒^{β0+β1𝑋+β2𝑋2...βp𝑋p}}{1+𝑒^{β0+β1𝑋+β2𝑋2+...βp𝑋p}}\n",
    " \\end{equation*}\n",
    " \n",
    "we use the maximum likelihood method to estimate β0,β1,...,βp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `getindex(df::DataFrame, col_inds::Union{AbstractVector, Regex, Not})` is deprecated, use `df[:, col_inds]` instead.\n",
      "│   caller = top-level scope at In[46]:4\n",
      "└ @ Core In[46]:4\n",
      "┌ Warning: `getindex(df::DataFrame, col_ind::ColumnIndex)` is deprecated, use `df[!, col_ind]` instead.\n",
      "│   caller = top-level scope at In[46]:8\n",
      "└ @ Core In[46]:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150-element Array{String,1}:\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " ⋮\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RDatasets: dataset\n",
    "\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "\n",
    "# ScikitLearn.jl expects arrays, but DataFrames can also be used - see\n",
    "# the corresponding section of the manual\n",
    "X = convert(Array, iris[[:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]])\n",
    "y = convert(Array, iris[:Species])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant LogisticRegression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "# This model requires scikit-learn. See\n",
    "# http://scikitlearnjl.readthedocs.io/en/latest/models/#installation\n",
    "@sk_import linear_model: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LogisticRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "fit!(model, X, y)\n",
    "\n",
    "accuracy = sum(predict(model, X) .== y) / length(y)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis and Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression involves directly modeling Pr(Y = k|X = x) using the logistic function for the case of two response classes. In statistical jargon, we model the conditional distribution of the response Y , given the predictor(s) X. We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors X separately in each of the response classes (i.e. given Y ), and then use Bayes’ theorem to flip these around into estimates for Pr(Y = k|X = x). When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Why do we need another method, when we have logistic regression? There are several reasons:\n",
    "- When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discrimi- nant analysis does not suffer from this problem.\n",
    "- If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n",
    "- linear discriminant analysis is popular when we have more than two response classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import discriminant_analysis: (LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject QuadraticDiscriminantAnalysis()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "model2= QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "fit!(model, X, y)\n",
    "\n",
    "accuracy = sum(predict(model, X) .== y) / length(y)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier. Given a positive integer K and a test observation x0, the KNN classifier first identifies the K points in the training data that are closest to x0, represented by N0. It then estimates the conditional probability for class j as the fraction of points in N0 whose response values equal j:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"knn.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, KNN applies Bayes rule and classifies the test observation x0 to the class with the largest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors (KNN) algorithm uses ‘feature similarity’ to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps −\n",
    "\n",
    "- Step 1 − For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data.\n",
    "\n",
    "- Step 2 − Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.\n",
    "\n",
    "- Step 3 − For each point in the test data do the following −\n",
    "\n",
    "     - 3.1 − Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean.\n",
    "\n",
    "     - 3.2 − Now, based on the distance value, sort them in ascending order.\n",
    "\n",
    "     - 3.3 − Next, it will choose the top K rows from the sorted array.\n",
    "\n",
    "     - 3.4 − Now, it will assign a class to the test point based on most frequent class of these rows.\n",
    "\n",
    "- Step 4 − End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.neighbors._classification.KNeighborsClassifier'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import neighbors: KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "fit!(model, X, y)\n",
    "\n",
    "accuracy = sum(predict(model, X) .== y) / length(y)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data. The matrix is NxN, where N is the number of target values (classes). Performance of such models is commonly evaluated using the data in the matrix. The following table displays a 2x2 confusion matrix for two classes (Positive and Negative).\n",
    "\n",
    "<img src=\"confusion.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy : the proportion of the total number of predictions that were correct.\n",
    "- Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "- Negative Predictive Value : the proportion of negative cases that were correctly identified.\n",
    "- Sensitivity or Recall : the proportion of actual positive cases which are correctly identified. \n",
    "- Specificity : the proportion of actual negative cases which are correctly identified. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "\n",
    "<img src=\"Example.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Chart\n",
    "\n",
    "The ROC chart is similar to the gain or lift charts in that they provide a means of comparison between classification models. The ROC chart shows false positive rate (1-specificity) on X-axis, the probability of target=1 when its true value is 0, against true positive rate (sensitivity) on Y-axis, the probability of target=1 when its true value is 1. Ideally, the curve will climb quickly toward the top-left meaning the model correctly predicted the cases. The diagonal red line is for a random model.\n",
    "\n",
    "<img src=\"ROC.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Curve (AUC)\n",
    "Area under ROC curve is often used as a measure of quality of the classification models. A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. In practice, most of the classification models have an AUC between 0.5 and 1.\n",
    "\n",
    "<img src=\"AUC.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling approaches can be computationally expensive, because they involve fitting the same statistical method multiple times using different subsets of the training data. However, due to recent advances in computing power, the computational requirements of resampling methods generally are not prohibitive. Here, we discuss two of the most commonly used resampling methods and  cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model is knownas model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. The training error can be easily calculated by applying the statistical learning method to the observations used in its training. But, the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Validation Set Approach\n",
    "\n",
    "Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach, is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate typically assessed using MSE in the case of a quantitative response provides an estimate of the test error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"validation.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `getindex(df::DataFrame, col_inds::Union{AbstractVector, Regex, Not})` is deprecated, use `df[:, col_inds]` instead.\n",
      "│   caller = top-level scope at In[56]:4\n",
      "└ @ Core In[56]:4\n",
      "┌ Warning: `getindex(df::DataFrame, col_ind::ColumnIndex)` is deprecated, use `df[!, col_ind]` instead.\n",
      "│   caller = top-level scope at In[56]:8\n",
      "└ @ Core In[56]:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150-element Array{String,1}:\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " ⋮\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RDatasets: dataset\n",
    "\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "\n",
    "# ScikitLearn.jl expects arrays, but DataFrames can also be used - see\n",
    "# the corresponding section of the manual\n",
    "X = convert(Array, iris[[:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]])\n",
    "y = convert(Array, iris[:Species])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set.\n",
    "\n",
    "Training Set: 70 %\n",
    "\n",
    "Testing Set: 30 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Array,1}:\n",
       " [5.0 2.0 3.5 1.0; 6.5 3.0 5.5 1.8; … ; 7.7 3.8 6.7 2.2; 4.6 3.2 1.4 0.2]\n",
       " [5.8 2.8 5.1 2.4; 6.0 2.2 4.0 1.0; … ; 5.0 3.5 1.6 0.6; 5.4 3.7 1.5 0.2]\n",
       " [\"versicolor\", \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"versicolor\", \"virginica\", \"versicolor\", \"versicolor\", \"virginica\"  …  \"versicolor\", \"versicolor\", \"versicolor\", \"setosa\", \"setosa\", \"setosa\", \"virginica\", \"versicolor\", \"virginica\", \"setosa\"]\n",
       " [\"virginica\", \"versicolor\", \"setosa\", \"virginica\", \"setosa\", \"virginica\", \"setosa\", \"versicolor\", \"versicolor\", \"versicolor\"  …  \"versicolor\", \"setosa\", \"versicolor\", \"versicolor\", \"versicolor\", \"virginica\", \"setosa\", \"virginica\", \"setosa\", \"setosa\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant LogisticRegression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "# This model requires scikit-learn. See\n",
    "# http://scikitlearnjl.readthedocs.io/en/latest/models/#installation\n",
    "@sk_import linear_model: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LogisticRegression()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LogisticRegression()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105-element Array{Any,1}:\n",
       " \"versicolor\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"versicolor\"\n",
       " \"virginica\"\n",
       " \"versicolor\"\n",
       " \"versicolor\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " ⋮\n",
       " \"setosa\"\n",
       " \"versicolor\"\n",
       " \"versicolor\"\n",
       " \"virginica\"\n",
       " \"versicolor\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"virginica\"\n",
       " \"versicolor\"\n",
       " \"virginica\"\n",
       " \"setosa\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=predict(model, X_test)\n",
    "y_predtrain=predict(model, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9809523809523809\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(predict(model, X_train) .== y_train) / length(y_train)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: confusion_matrix not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: confusion_matrix not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[63]:1"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_predtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(predict(model, X_test) .== y_test) / length(y_test)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: confusion_matrix not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: confusion_matrix not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[65]:1"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set approach is conceptually simple and is easy to imple- ment. But it has two potential drawbacks:\n",
    "\n",
    "1. The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "2. In the validation approach, only a subset of the observations those that are included in the training set rather than in the validation set are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out cross-validation (LOOCV) is closely related to the validation\n",
    "leave-one- set approach of Section 5.1.1, but it attempts to address that method’s out\n",
    "drawbacks.\n",
    "\n",
    "Like the validation set approach, LOOCV involves splitting the set of\n",
    "observations into two parts. However, instead of creating two subsets of\n",
    "comparable size, a single observation (x1,y1) is used for the validation\n",
    "set, and the remaining observations {(x2, y2), . . . , (xn, yn)} make up the\n",
    "training set. The statistical learning method is fit on the n − 1 training\n",
    "observations, and a prediction yˆ is made for the excluded observation, 1\n",
    "using its value x1. Since (x1, y1) was not used in the fitting process, MSE provides an approximately unbiased estimate for the test error.\n",
    "But even though MSE1 is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation (x1,y1).\n",
    "\n",
    "<img src=\"loocv.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the procedure by selecting (x2,y2) for the validation data, training the statistical learning procedure on the n − 1 observations {(x ,y ),(x ,y ),...,(x ,y )}, and computing MSE2 Repeating this approach n times produces n squared errors, MSE1,..., MSEn. \n",
    "\n",
    "The LOOCV estimate for the test MSE is the average of these n test error estimates:\n",
    "\n",
    "<img src=\"cv.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV has a couple of major advantages over the validation set approach. \n",
    "\n",
    "1. It has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain n − 1 observations, almost as many as are in the entire data set.\n",
    "2. To the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. k-Fold Cross-Validation\n",
    "\n",
    "<img src=\"kfold.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. This process results in k estimates of the test error, MSE1, MSE2, . . . , MSEk. \n",
    "\n",
    "The k-fold CV estimate is computed by averaging these values,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cvk.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious advantage is computational. LOOCV requires fitting the statistical learning method n times. This has the potential to be computationally expensive. But cross-validation is a very general approach that can be applied to almost any statistical learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant LinearRegression\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using ScikitLearn.CrossValidation: cross_val_predict\n",
    "\n",
    "@sk_import datasets: load_boston\n",
    "@sk_import linear_model: LinearRegression\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "boston = load_boston()\n",
    "y = boston[\"target\"]\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validated:\n",
    "predicted = cross_val_predict(lr, boston[\"data\"], y, cv=10)\n",
    "\n",
    "PyPlot.scatter(y, predicted)\n",
    "PyPlot.plot([minimum(y), maximum(y)], [minimum(y), maximum(y)], \"k--\", lw=4)\n",
    "xlabel(\"Measured\")\n",
    "ylabel(\"Predicted\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off for k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that bias is not the only source for concern in an esti- mating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k < n. Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. \n",
    "\n",
    "In contrast, when we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. \n",
    "\n",
    "Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\n",
    "\n",
    "To summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.PyPlotBackend()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using PyCall\n",
    "using PyPlot, Printf\n",
    "using ScikitLearn.CrossValidation: train_test_split\n",
    "using Statistics, Random\n",
    "using DataFrames\n",
    "using CSV\n",
    "using StatsPlots\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant LinearRegression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._base.LinearRegression'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn, PyPlot, Random, Statistics, Printf\n",
    "using ScikitLearn.CrossValidation: cross_val_score\n",
    "using ScikitLearn.Pipelines: Pipeline\n",
    "\n",
    "@sk_import preprocessing: PolynomialFeatures\n",
    "@sk_import linear_model: LinearRegression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 1400x500 with 3 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Random.seed!(2)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "true_fun(X) = cos.(1.5 * pi * X)\n",
    "X = sort(rand(n_samples))\n",
    "y = true_fun(X) + randn(n_samples) * 0.1\n",
    "\n",
    "figure(figsize=(14, 5))\n",
    "for (i, degree) in enumerate(degrees)\n",
    "    ax = subplot(1, length(degrees), i)\n",
    "    setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=false)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    fit!(pipeline, hcat(X), y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, hcat(X), y, scoring=\"mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = range(0, stop=1, length=100)\n",
    "    PyPlot.plot(X_test, predict(pipeline, hcat(X_test)), label=\"Model\")\n",
    "    PyPlot.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    PyPlot.plot(X, y, label=\"Samples\")\n",
    "    xlabel(\"x\")\n",
    "    ylabel(\"y\")\n",
    "    xlim((0, 1))\n",
    "    ylim((-2, 2))\n",
    "    legend(loc=\"best\")\n",
    "    title(@sprintf(\"Degree %d\\nMSE = %.2e +/- %.2e\", degree, -mean(scores), std(scores)))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection And Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression setting, the standard linear model\n",
    "    \n",
    "    Y =β0 +β1X1 +···+βpXp +ε \n",
    "\n",
    "is commonly used to describe the relationship between a response Y and a set of variables X1,X2,...,Xp. that one typically fits this model using least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the linear model has distinct advantages in terms of inference and, on real-world problems, is often surprisingly competitive in relation to non-linear methods. Hence, before moving to the non-linear world, we discuss in this chapter some ways in which the simple linear model can be improved, by replacing plain least squares fitting with some alternative fitting procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might we want to use another fitting procedure instead of least squares? As we will see, alternative fitting procedures can yield better prediction accuracy and model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prediction Accuracy:\n",
    "Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. \n",
    "\n",
    "If n ≫ p—that is, if n, the number of observations, is much larger than p, the number of variables—then the least squares estimates tend to also have low variance, and hence will perform well on test observations. However, if n is not much larger than p, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. And if p > n, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all. \n",
    "\n",
    "By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Interpretability.\n",
    "\n",
    "It is often the case that some or many of the\n",
    "variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. \n",
    "\n",
    "By removing these\n",
    "variables that is, by setting the corresponding coefficient estimates\n",
    "to zero we can obtain a model that is more easily interpreted. Now\n",
    "least squares is extremely unlikely to yield any coefficient estimates\n",
    "that are exactly zero.  \n",
    "\n",
    "We see some approaches for automatically performing feature selection or variable selection—that is,\n",
    "for excluding irrelevant variables from a multiple regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many alternatives, both classical and modern, to using least squares to fit the model. Here we discuss three important classes of methods.\n",
    "- Subset Selection. This approach involves identifying a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\n",
    "\n",
    "\n",
    "-  Shrinkage. This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.\n",
    "\n",
    "\n",
    "-  Dimension Reduction. This approach involves projecting the p predictors into a M-dimensional subspace, where M < p. This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subset Selection:\n",
    "\n",
    "We consider some methods for selecting subsets of predictors. These include best subset and stepwise model selection procedures.\n",
    "\n",
    "### A. Best Subset Selection\n",
    "To perform best subset selection, we fit a separate least squares regression\n",
    "for each possible combination of the p predictors.That is, we fit all p models    \n",
    "that contain exactly one predictor, all 􏰁\n",
    "\\begin{equation*}\n",
    "\\binom{p}{2}=\\frac{p(p−1)}{2}\n",
    "\\end{equation*}     \n",
    "models that contain 2\n",
    "exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best.\n",
    "\n",
    "The problem of selecting the best model from among the 2p possibilities considered by best subset selection is not trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 6.1 Best subset selection\n",
    "1. Let M0 denote the null model, which contains no predictors. This\n",
    "model simply predicts the sample mean for each observation.\n",
    "2. For  k=1,2,...p:\n",
    "  - Fit all 􏰁p􏰀 models that contain exactly k predictors. k 􏰁p􏰀\n",
    "  - Pick the best among these k models, and call it Mk. Here best 2\n",
    "is defined as having the smallest RSS, or equivalently largest R .\n",
    "3. Select a single best model from among M0, . . . , Mp using cross validated prediction error, Cp (AIC), BIC, or adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Stepwise Selection\n",
    "\n",
    "For computational reasons, best subset selection cannot be applied with very large p. Best subset selection may also suffer from statistical problems when p is large. The larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Forward Stepwise Selection\n",
    "Forward stepwise selection is a computationally efficient alternative to best subset selection.\n",
    "Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 6.2 Forward stepwise selection\n",
    "\n",
    "1. Let M0 denote the null model, which contains no predictors.\n",
    "2. For  k = 0,...,p−1:\n",
    "  - Consider all p − k models that augment the predictors in Mk\n",
    "with one additional predictor.\n",
    "  - Choose the best among these p − k models, and call it Mk+1 Here best is defined as having smallest RSS or highest $R^2$\n",
    "3. Select a single best model from among M0, . . . , Mp using cross validated prediction error, Cp (AIC), BIC, or adjusted $R^2$ .\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Backward Stepwise Selection\n",
    "Backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let M0 denote the null model, which contains no predictors.\n",
    "2. For k = p,p−1,...,1:\n",
    "  - Consider all k models that contain all but one of the predictors in Mk, for a total of k − 1 predictors.\n",
    "  - Choose the best among these k models, and call it Mk−1. Here best is defined as having smallest RSS or highest  $R^2$\n",
    "3. Select a single best model from among M0, . . . , Mp using cross validated prediction error, Cp (AIC), BIC, or adjusted $R^2$ .\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Optimal Model\n",
    "\n",
    "Best subset selection, forward selection, and backward selection result in the creation of a set of models, each of which contains a subset of the p predictors. In order to implement these methods, we need a way to determine which of these models is best. The model containing all of the predictors will always have the smallest RSS and the largest R2, since these quantities are related to the training error. Instead, we wish to choose a model with a low test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\n",
    "1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n",
    "2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cp, AIC, BIC, and Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set MSE is generally an under- estimate of the test MSE. (Recall that MSE = RSS/n.) This is because when we fit a model to the training data using least squares, we specifi- cally estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. \n",
    "\n",
    "The training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.\n",
    "\n",
    "However, a number of techniques for adjusting the training error for the model size are available. These approaches can be used to select among a set of models with different numbers of variables. We now consider four such approaches: Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted $R^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cp :\n",
    "\n",
    "For a fitted least squares model containing d predictors, the C estimate of test MSE is computed using the equation\n",
    "\n",
    "\\begin{equation*}\n",
    "Cp=\\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\n",
    "\\end{equation*} \n",
    "\n",
    "where σˆ2 is an estimate of the variance of the error ε associated with each response measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Cp statistics adds a penalty of $2d\\hat{\\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. Clearly, the penalty increases as the number of predictors in the model increases; this is intended to adjust for the corresponding decrease in training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AIC\n",
    "The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model with Gaussian errors, maximum likelihood and least squares are the same thing.\n",
    "\n",
    "\\begin{equation*}\n",
    "AIC=\\frac{1}{n\\hat{\\sigma}^2}(RSS+2d\\hat{\\sigma}^2)\n",
    "\\end{equation*}\n",
    "\n",
    "where, for simplicity, we have omitted an additive constant. Hence for least squares models, Cp and AIC are proportional to each other, and so only Cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BIC\n",
    "BIC is derived from a Bayesian point of view, but ends up looking similar to Cp (and AIC) as well. For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by\n",
    "\n",
    "\\begin{equation*}\n",
    "BIC=\\frac{1}{n}(RSS+log(n)d\\hat{\\sigma}^2)\n",
    "\\end{equation*}\n",
    "\n",
    "Like Cp, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adjusted $R^2$\n",
    "\n",
    "The adjusted $R^2$ statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall that the usual $R^2$ is defined as 1 − RSS/TSS, where TSS is the total sum of squares for the response. \n",
    "\n",
    "Since RSS always decreases as more variables are added to the model, the $R^2$ always increases as more variables are added. For a least squares model with d variables, the adjusted $R^2$ statistic is calculated as\n",
    "\n",
    "<img src=\"rsquare.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Cp, AIC, and BIC, for which a small value indicates a model with\n",
    "a low test error, a large value of adjusted R2 indicates a model with a\n",
    "small test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation and Cross-Validation\n",
    "\n",
    "We can compute the validation set error or the\n",
    "cross-validation error for each model under consideration, and then select\n",
    "the model for which the resulting estimated test error is smallest. This pro-\n",
    "cedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that\n",
    "it provides a direct estimate of the test error, and makes fewer assumptions\n",
    "about the true underlying model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shrinkage Methods:\n",
    "The subset selection methods described in Section 6.1 involve using least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\n",
    "\n",
    "It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ridge Regression\n",
    "\n",
    "The least squares fitting procedure estimates β0,β1,...,βp using the values that minimize\n",
    "<img src=\"ridge1.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates βˆR are the values that minimize.\n",
    "<img src=\"ridge2.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where λ ≥ 0 is a tuning parameter, to be determined separately. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS\n",
    "small.\n",
    "\n",
    "However, the second term, $λ\\sum (βj)^2$, called a shrinkage penalty, is \n",
    "small when β1, . . . , βp are close to zero, and so it has the effect of shrinking\n",
    "the estimates of βj towards zero.\n",
    "\n",
    "When λ = 0, the penalty term has no effect, and ridge regression\n",
    "will produce the least squares estimates. However, as λ → ∞, the impact of\n",
    "the shrinkage penalty grows, and the ridge regression coefficient estimates\n",
    "will approach zero.\n",
    "\n",
    "The shrinkage penalty is applied to β1,...,βp, but not to the intercept β0. We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when xi1 = xi2 = . . . = xip = 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does Ridge Regression Improve Over Least Squares?\n",
    "\n",
    "Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n",
    "\n",
    "At the least squares coefficient estimates, which correspond to ridge regression with λ = 0, the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant load_boston\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using ScikitLearn.CrossValidation: cross_val_predict\n",
    "\n",
    "@sk_import datasets: load_boston\n",
    "@sk_import linear_model: Ridge\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506-element Array{Float64,1}:\n",
       "  39.96980566904678\n",
       "   8.443767817181927\n",
       "  18.92896768991901\n",
       "  22.28020316152497\n",
       "  70.15819554965859\n",
       "  12.457306740196957\n",
       "   0.05455119021032788\n",
       "  71.88353102898981\n",
       "  40.061413715727205\n",
       "   0.5062905694049087\n",
       "   9.111765585712204\n",
       "   4.635427281702125\n",
       "   0.9907838202520087\n",
       "   ⋮\n",
       "  15.642419333274932\n",
       "  41.59121545784503\n",
       "  35.044754631201045\n",
       "   0.7728481716452814\n",
       "   0.020344850030622728\n",
       "   0.8464998245276442\n",
       "  13.824096173243555\n",
       "   3.0200858687439043\n",
       "   5.708616920358086\n",
       "  20.671071248993307\n",
       "  24.283265236351244\n",
       " 124.220375437099"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Ridge()\n",
    "boston = load_boston()\n",
    "y = boston[\"target\"]\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validated:\n",
    "predicted = cross_val_predict(lr, boston[\"data\"], y, cv=10)\n",
    "\n",
    "performance_testdf = DataFrame(y=y, y_predicted = predicted)\n",
    "performance_testdf.error = performance_testdf[!,:y] - performance_testdf[!,:y_predicted]\n",
    "performance_testdf.error_sq = performance_testdf.error.*performance_testdf.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapetest (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAPE function defination\n",
    "function mapetest(performance_df)\n",
    "    mape = mean(abs.(performance_df.error./performance_df.y))\n",
    "    return mape\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute test error: 3.910378746409421\n",
      "\n",
      "Mean Aboslute Percentage test error: 0.20820321173554623\n",
      "\n",
      "Root mean square test error: 5.822931339632328\n",
      "\n",
      "Mean square test error: 33.906529386072336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Error\n",
    "println(\"Mean Absolute test error: \",mean(abs.(performance_testdf.error)), \"\\n\")\n",
    "println(\"Mean Aboslute Percentage test error: \",mapetest(performance_testdf), \"\\n\")\n",
    "println(\"Root mean square test error: \",rmse(performance_testdf), \"\\n\")\n",
    "println(\"Mean square test error: \",mean(performance_testdf.error_sq), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso Regression\n",
    "Ridge regression does have one obvious disadvantage. Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model. \n",
    "\n",
    "The penalty $λ\\sum (βj)^2$  will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless λ = ∞). This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. \n",
    "\n",
    "<img src=\"lasso.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that the $(βj)^2$ term in the ridge regression penalty has been replaced by |βj| in the lasso penalty. In statistical parlance, the lasso uses an l1 (pronounced “ell 1”) penalty instead of an l2 penalty. The l1 norm of a coefficient vector β is given by $∥β∥1 =􏰂 \\sum|βj|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the l1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Hence, much like best subset se- lection, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields sparse models—that is, models that involve only a subset of the variables. As in ridge regression, selecting a good value of λ for the lasso is critical; where we use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant load_boston\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using ScikitLearn.CrossValidation: cross_val_predict\n",
    "\n",
    "@sk_import datasets: load_boston\n",
    "@sk_import linear_model: Lasso\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506-element Array{Float64,1}:\n",
       "  51.041787535796864\n",
       "  18.17859386301951\n",
       "  20.094028868734696\n",
       "  13.586702825427103\n",
       "  64.11821312507242\n",
       "   1.1453981203736632\n",
       "   1.8869110344998268\n",
       "  57.51145393440651\n",
       "  36.273732130925076\n",
       "   1.5884511367177814\n",
       "  11.11812040915001\n",
       "  20.687402525156376\n",
       "   0.3790451528579222\n",
       "   ⋮\n",
       "  17.583427580858864\n",
       "  47.96996122371177\n",
       "  28.523233684669535\n",
       "   3.5104125527401493\n",
       "   0.12765713270947623\n",
       "   4.296711184340147\n",
       "  16.082979775549862\n",
       "   3.3649548769855455\n",
       "  15.534811005859721\n",
       "  23.010954270184254\n",
       "  31.278830921019793\n",
       " 180.31628741166324"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Lasso()\n",
    "boston = load_boston()\n",
    "y = boston[\"target\"]\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validated:\n",
    "predicted = cross_val_predict(lr, boston[\"data\"], y, cv=10)\n",
    "\n",
    "performance_testdf = DataFrame(y=y, y_predicted = predicted)\n",
    "performance_testdf.error = performance_testdf[!,:y] - performance_testdf[!,:y_predicted]\n",
    "performance_testdf.error_sq = performance_testdf.error.*performance_testdf.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute test error: 4.066199877131211\n",
      "\n",
      "Mean Aboslute Percentage test error: 0.19544252543729593\n",
      "\n",
      "Root mean square test error: 5.867529873320226\n",
      "\n",
      "Mean square test error: 34.427906814305274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Error\n",
    "println(\"Mean Absolute test error: \",mean(abs.(performance_testdf.error)), \"\\n\")\n",
    "println(\"Mean Aboslute Percentage test error: \",mapetest(performance_testdf), \"\\n\")\n",
    "println(\"Root mean square test error: \",rmse(performance_testdf), \"\\n\")\n",
    "println(\"Mean square test error: \",mean(performance_testdf.error_sq), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods that we have discussed so far in this chapter have controlled variance in two different ways, either by using a subset of the original variables, or by shrinking their coefficients toward zero.\n",
    "\n",
    "All of these methods signal are defined using the original predictors, X1,X2,...,Xp. We now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these tech- niques as dimension reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Z1 , Z2 , . . . , ZM represent M < p linear combinations of our original p predictors. That is,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Zm=\\sum_{i=1}^{p}\\phi_{jm}X_{j}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for some constants $\\phi_{1m}$,$\\phi_{2m}$ ...,$\\phi_{pm}$, m = 1,...,M.  We can then fit the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "y_{i}=\\theta_{0}+\\sum_{m=1}^{M}\\theta_{m}z_{im} + εi,           i=1,...,n\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using least squares. The regression coefficients are given by $\\theta_{0}$,$\\theta_{0}$, ..., $\\theta_{M}$. If the constants $\\phi_{1m}$,$\\phi_{2m}$ ...,$\\phi_{pm}$ are  chosen wisely,then such dimension reduction approaches can often outperform least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The term dimension reduction comes from the fact that this approach reduces the problem of estimating the p+1 coefficients β0,β1,...,βp to the simpler problem of estimating the M + 1 coefficients θ0, θ1, . . . , θM , where M < p. \n",
    "\n",
    "In other words, the dimension of the problem has been reduced from p+1 to M +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dim.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "where\n",
    "\\begin{equation*}\n",
    "\\beta_{j}=\\sum_{m=1}^{M}\\phi_{jm}\\theta_{m}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction serves to constrain the estimated βj coefficients, since now they must take the form. This constraint on the form of the coefficients has the potential to bias the coefficient estimates. However, in situations where p is large relative to n, selecting a value of M ≪ p can significantly reduce the variance of the fitted coefficients. If M = p, and all the Zm are linearly independent, then poses no constraints. In this case, no dimension reduction occurs, and so fitting above euation is equivalent to performing least squares on the original p predictors.\n",
    "\n",
    "All dimension reduction methods work in two steps. First, the trans- formed predictors Z1, Z2, . . . , ZM are obtained. Second, the model is fit using these M predictors. However, the choice of Z1, Z2, . . . , ZM , or equiv- alently, the selection of the φjm’s, can be achieved in different ways. In this chapter, we will consider two approaches for this task: principal components and partial least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components analysis (PCA) is a popular approach for deriving a low-dimensional set of features from a large set of variables. PCA is a technique for reducing the dimension of a n × p data matrix X. The first principal component direction of the data is that along which the observations vary the most. \n",
    "\n",
    "<img src=\"pca1.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we projected the 100 observations onto this line, then the resulting projected observations would have the largest possible variance; projecting the observations onto any other line would yield projected observations with lower variance. Projecting a point onto a line simply involves finding the location on the line which is closest to the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pca2.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "There is also another interpretation for PCA: the first principal compo- nent vector defines the line that is as close as possible to the data. For instance, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line. These distances are plotted as dashed line segments in the left-hand panel, in which the crosses represent the projection of each point onto the first principal component line. The first principal component has been chosen so that the projected observations are as close as possible to the original observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares\n",
    "PLS is a dimension reduction method,  which first identifies a new set of features Z1,...,ZM that are linear combinations of the original features, and then fits a linear model via least squares using these M new features. But unlike PCR, PLS identifies these new features in a supervised way—that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response. Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now describe how the first PLS direction is computed. After standardizing the p predictors, PLS computes the first direction Z1 by setting each φj1 in (6.16) equal to the coefficient from the simple linear regression of Y onto Xj. One can show that this coefficient is proportional to the cor- relation between Y and Xj. Hence, in computing $Z_{1} =\\sum_{􏰂j=1}^{p} \\phi_{j1}X_{j}$, PLS places the highest weight on the variables that are most strongly related to the response.\n",
    "\n",
    "<img src=\"pls.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid green line indicates the first PLS direction, while the dotted line shows the first principal component direction. PLS has chosen a direction that has less change in the ad dimension per unit change in the pop dimension, relative\n",
    "partial least squares to PCA. This suggests that pop is more highly correlated with the response than is ad. The PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the second PLS direction we first adjust each of the variables for Z1, by regressing each variable on Z1 and taking residuals. These resid- uals can be interpreted as the remaining information that has not been explained by the first PLS direction. We then compute Z2 using this or- thogonalized data in exactly the same fashion as Z1 was computed based on the original data. This iterative approach can be repeated M times to identify multiple PLS components Z1,...,ZM. Finally, at the end of this procedure, we use least squares to fit a linear model to predict Y using Z1,...,ZM in exactly the same fashion as for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with PCA, the number M of partial least squares directions used in PLS is a tuning parameter that is typically chosen by cross-validation. We generally standardize the predictors and response before performing PLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the supervised dimension reduction of PLS can reduce bias, it also has the potential to increase variance, so that the overall benefit of PLS relative to PCA is a wash.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Beyond Linearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are relatively simple to describe and implement, and have advantages over other approaches in terms of interpretation and inference. However, standard linear regression can have significant limitations in terms of predictive power. This is because the linearity assumption is almost always an approximation, and sometimes a poor one. we see that we can improve upon least squares using ridge regression, the lasso, principal com- ponents regression, and other techniques. In that setting, the improvement is obtained by reducing the complexity of the linear model, and hence the variance of the estimates. \n",
    "\n",
    "We relax the linearity assumption while still attempting to maintain as much interpretability as possible. We do this by examining very simple extensions of linear models like polynomial regression and step functions, as well as more sophisticated approaches such as splines, local regression, and generalized additive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. For example, a cubic regression uses three variables, X, X2, and X3, as predictors.\n",
    "- Step functions cut the range of a variable into K distinct regions in order to produce a qualitative variable.\n",
    "- Regression splines are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of X into K distinct regions. Within each region, a polynomial function is fit to the data\n",
    "- Smoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\n",
    "- Local regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\n",
    "- Generalized additive models allow us to extend the methods above to deal with multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Polynomial Regression\n",
    "\n",
    "The standard way to extend linear regression to settings in which the relationship between the predictors and the response is non- linear has been to replace the standard linear model\n",
    "\n",
    "$ y_{i} = β_{0} + β_{1}x_{i} + ε_{i}$\n",
    "     \n",
    "with a polynomial function\n",
    "\n",
    "$y_{i} = β_{0} + β_{1}x_{i} +β_{2}x_{i}^{2} +β_{3}x_{i}^3 +... +ε_{i}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where εi is the error term. This approach is known as polynomial regression. For large\n",
    "enough degree d, a polynomial regression allows us to produce an extremely\n",
    "non-linear curve.the coefficients in above equation can be easily estimated\n",
    "using least squares linear regression because this is just a standard linear model with predictors $x_i, x_{i}^{2} , x_{i}^{3} , . . . , x_{i}^{d}$.  Generally speaking, it is unusual\n",
    "to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible and can take on some very strange shapes. This is especially true near the boundary of the X variable. \n",
    "\n",
    "<img src=\"poly.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step Functions\n",
    "Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of X. We can instead use step functions in order to avoid imposing such a global structure. Here we break the range of X into bins, and fit a different constant in each bin. This amounts to converting a continuous variable into an ordered categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In greater detail, we create cutpoints $c_{1}, c_{2}, . . . , c_{K}$ in the range of X, and then construct K + 1 new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_{0}(X) = I(X < c_{1})$\n",
    "\n",
    "$C_{1}(X) = I(c_{1} <= X < c_{2})$\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "$C_{k}(X) = I(c_{k}<= X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"step.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where I(·) is an indicator function that returns a 1 if the condition is true, and returns a 0 otherwise. These are sometimes called dummy variables. Notice that for any value of X, $C_{0}(X)+C_{1}(X)+...+C_{K}(X) = 1$, since X must be in exactly one of the K + 1 intervals. We then use least squares to fit a linear model using $C_{1}(X), C_{2}(X), . . . , C_{K} (X)$ as predictors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{i} = β_{0} + β_{1}C_{1}(x_{i}) + β_{2}C_{2}(x_{i}) + ... + β_{K}C_{K}(x_{i}) + ε_{i}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given value of X, at most one of $C_{1}, C_{2}, . . . , C_{K}$ can be non-zero. Note that when X < c1, all of the predictors in equation are zero, so β0 can be interpreted as the mean value of Y for X < c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basis Functions\n",
    "Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach. The idea is to have at hand a fam-\n",
    "ily of functions or transformations that can be applied to a variable X: $b_{1}(X), b_{2}(X), . . . , b_{K} (X)$. Instead of fitting a linear model in X, we fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     yi = β0 + β1b1(xi) + β2b2(xi) + β3b3(xi) + ... + βKbK(xi) + εi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the basis functions b1(·),b2(·),...,bK(·) are fixed and known. (In other words, we choose the functions ahead of time.) For polynomial regression, the basis functions are $b_{j}(xi) = x_{i}^{j}$, and for piecewise constant functions they are $b_{j}(xi) = I(cj ≤ xi < cj+1)$. We can think of (7.7) as a standard linear model with predictors b1(xi),b2(xi),...,bK(xi). Hence, we can use least squares to estimate the unknown regression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regression Splines\n",
    "Now we discuss a flexible class of basis functions that extends upon the polynomial regression and piecewise constant regression approaches that we have just seen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Piecewise Polynomials\n",
    "Instead of fitting a high-degree polynomial over the entire range of X, piece- wise polynomial regression involves fitting separate low-degree polynomials over different regions of X.Forexample,apiecewisecubicpolynomialworks by fitting a cubic regression model of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{i} =β_{0} +β_{1}x_{i} +β_{2}x_{i}^{2} +β_{3}x_{i}^{3} +ε_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the coefficients β0, β1, β2, and β3 differ in different parts of the range of X. The points where the coefficients change are called knots.\n",
    "\n",
    "A piecewise cubic polynomial with a single knot at a point c takes the form\n",
    "<img src=\"piece.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we fit two different polynomial functions to the data, one on the subset of the observations with xi < c, and one on the subset of the observations with xi ≥ c. The first polynomial function has coefficients $β_{01},β_{11},β_{21},β_{31}$, and the second has coefficients $β_{02},β_{12},β_{22},β_{32}$. Each of these polynomial functions can be fit using least squares applied to simple functions of the original predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using more knots leads to a more flexible piecewise polynomial. In gen- eral, if we place K different knots throughout the range of X, then we will end up fitting K + 1 different cubic polynomials. Note that we do not need to use a cubic polynomial.\n",
    "\n",
    "<img src=\"pieceandcon.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Constraints and Splines\n",
    "The top left panel of Figure  looks wrong because the fitted curve is just too flexible. To remedy this problem, we can fit a piecewise polynomial under the constraint that the fitted curve must be continuous.\n",
    "\n",
    "we have added two additional constraints: now both the first and second derivatives of the piecewise polynomials are continuous\n",
    "\n",
    "The curve in the bottom left derivative plot is called a cubic spline. a total of 4 + K degrees of freedom.  The general definition of a degree-d spline is that it is a piecewise degree-d polynomial, with continuity in derivatives up to degree d − 1 at each knot. Therefore, a linear spline is obtained by fitting a line in each region of the predictor space defined by the knots, requiring continuity at each knot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The Spline Basis Representation\n",
    "\n",
    "The regression splines that we just saw in the previous section may have seemed somewhat complex: how can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous? \n",
    "\n",
    "A cubic spline with K knots can be modeled as\n",
    "\n",
    "$y_{i} = β_{0} + β_{1}b_{1}(x_{i}) + β_{2}b_{2}(x_{i}) + · · · + β_{K+3}b_{K+3}(x_{i}) + ε_{i},$\n",
    "\n",
    "\n",
    "for an appropriate choice of basis functions b1,b2,...,bK+3. The model can then be fit using least squares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Number and Locations of the Knots\n",
    "When we fit a spline, where should we place the knots? The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable.\n",
    "    \n",
    "One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Smoothing Splines\n",
    "\n",
    "\n",
    "We create by specifying a set of knots, producing a sequence of basis functions, and then using least squares to estimate the spline coefficients. We now introduce a somewhat different approach that also produces a spline.\n",
    "\n",
    "In fitting a smooth curve to a set of data, what we really want to do is find some function, say g(x), that fits the observed data well: that is, we want $RSS = \\sum_􏰂{i=1}^{n}(yi − g(xi))^2$ to be small. However, there is a problem with this approach. If we don’t put any constraints on g(xi), then we can always make RSS zero simply by choosing g such that it interpolates all of the yi. Such a function would woefully overfit the data—it would be far too flexible. What we really want is a function g that makes RSS small, but that is also smooth.\n",
    "\n",
    "How might we ensure that g is smooth? There are a number of ways to do this. A natural approach is to find the function g that minimizes\n",
    "\n",
    "<img src=\"spline.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "λ is a nonnegative tuning parameter. The function g that minimizes equation is known as a smoothing spline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  first term in equation is a loss function that encourages g to fit the data well, and the second term in equation is a penalty term that penalizes the variability in g. The notation g′′(t) indicates the second\n",
    "derivative of the function g. The first derivative g′(t) measures the slope\n",
    "of a function at t, and the second derivative corresponds to the amount by\n",
    "which the slope is changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When λ = 0, then the penalty term in equation has no effect, and so the function g will be very jumpy and will exactly interpolate the training observations. When λ → ∞, g will be perfectly smooth—it will just be a straight line that passes as closely as possible to the training points. For an intermediate value of λ, g will approximate the training observations but will be somewhat smooth. We see that λ controls the bias-variance trade-off of the smoothing spline.\n",
    "\n",
    "<img src=\"splinefit.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "But the tuning parameter λ controls the roughness of the smoothing spline, and hence the effective degrees of freedom. It is possible to show that as λ increases from 0 to ∞, the effective degrees of freedom, which we write $df_{λ}$, decrease from n to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Local Regression\n",
    "\n",
    "Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x0 using only the nearby training observations.\n",
    " \n",
    "<img src=\"local.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "    \n",
    "In this figure the blue line represents the function f(x) from which the data were generated, and the light orange line corresponds to the local\n",
    "regression estimate $\\hat{f}(x)$. Local regression is described in Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm  \n",
    "### Local Regression At $X = x_{0}$\n",
    "1. Gather the fraction $s = k/n$ of training points whose xi are closest\n",
    "to $x_{0}$.\n",
    "2. Assign a weight $K_{i0} = K(x_{i},x_{0})$ to each point in this neighborhood, so that the point furthest from $x_{0}$ has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.\n",
    "3. Fit a weighted least squares regression of the yi on the xi using the afor mentioned weights, by finding $\\hat{β_{0}}$ and $\\hat{β_{1}}$ that minimize\n",
    "where\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^{n}K_{i0}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "4. The fitted value at x0 is given by $f(x_{0}) = \\hat{β_{0}} + \\hat{β_{0}}x_{0}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of local regression can be generalized in many different ways. In a setting with multiple features X1 , X2 , . . . , Xp , one very useful general- ization involves fitting a multiple linear regression model that is global in some variables, but local in another, such as time. Such varying coefficient models are a useful way of adapting a model to the most recently gathered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Generalized Additive Models\n",
    "\n",
    "Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables,while maintaining additivity.Justlikelinearmodels,GAMs can be applied with both quantitative and qualitative responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. GAMs for Regression Problems\n",
    "A natural way to extend the multiple linear regression model\n",
    "\n",
    "yi =β0 +β1xi1 +β2xi2 +···+βpxip +εi\n",
    "\n",
    "in order to allow for non-linear relationships between each feature and the response is to replace each linear component βjxij with a (smooth) non- linear function fj(xij). We would then write the model as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gam1.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "    \n",
    "\n",
    "This is an example of a GAM. It is called an additive model because we calculate a separate fj for each Xj, and then add together all of their contributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. GAMs for Classification Problems\n",
    "\n",
    "GAMs can also be used in situations where Y is qualitative. For simplicity, here we will assume Y takes on values zero or one, and let p(X) = Pr(Y = 1|X) be the conditional probability (given the predictors) that the response equals one. Recall the logistic regression model \n",
    "\n",
    "\\begin{equation*}\n",
    "log(\\frac{𝑝(𝑋)}{1 − p(X)})={β0+β1𝑋+β2𝑋2+β3𝑋3+β4𝑋4....βp𝑋p}\n",
    " \\end{equation*}\n",
    "\n",
    "    \n",
    "This logit is the log of the odds of P(Y = 1|X) versus P(Y = 0|X), which represents as a linear function of the predictors. A natural way to extend (7.17) to allow for non-linear relationships is to use the model\n",
    "            \n",
    " \\begin{equation*}\n",
    "log(\\frac{𝑝(𝑋)}{1 − p(X)})={β0+f_{1}(𝑋_{1})+f_{2}(𝑋_{2})+f_{3}(𝑋_{3})....f_{p}(𝑋_{p})}\n",
    " \\end{equation*}\n",
    "\n",
    "is a logistic regression GAM. It has all the same pros and cons as discussed in the previous section for quantitative responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs have been shown to perform well in a variety of settings, and are often considered one of the best “out of the box” classifiers.\n",
    "\n",
    "The support vector machine is a generalization of a simple and intu- itive classifier called the maximal margin classifier\n",
    "\n",
    "People often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as “support vector machines”. To avoid confusion, we will carefully distinguish between these three notions in this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "we define a hyperplane and introduce the concept of an optimal separating hyperplane.\n",
    "\n",
    "### What Is a Hyperplane?\n",
    "In a p-dimensional space, a hyperplane is a flat affine subspace of dimension p − 1\n",
    "\n",
    "\n",
    "In 2D, a hyperplane is a flat one-dimensional subspace in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In p > 3 dimensions, it can be hard to visualize a hyperplane, but the notion of a (p − 1)-dimensional flat subspace still applies.\n",
    "\n",
    "The mathematical definition of a hyperplane is quite simple. In two di- mensions, a hyperplane is defined by the equation\n",
    "\n",
    "$β_{0} + β_{1}X_{1} + β_{2}X_{2} = 0$\n",
    "\n",
    "for parameters β0, β1, and β2. is simply the equation of a line, since indeed in two dimensions a hyperplane is a line. Above equation can be easily extended to the p-dimensional setting:\n",
    "\n",
    "$β_{0} + β_{1}X_{1} + β_{2}X_{2} ... +β_{p}X_{p}= 0$\n",
    "\n",
    "defines a p-dimensional hyperplane, again in the sense that if a point $ X = (X1,X2,...,Xp)^{T}$ in p dimensional space(i.e.a vector of length p)satisfies above equation, then X lies on the hyperplane.\n",
    "\n",
    "Now, suppose that X does not satisfy rather,\n",
    "\n",
    "$β_{0} + β_{1}X_{1} + β_{2}X_{2} ... +β_{p}X_{p} >0$.\n",
    "\n",
    "Then this tells us that X lies to one side of the hyperplane. On the other hand, if\n",
    "\n",
    "$β_{0} + β_{1}X_{1} + β_{2}X_{2} ... +β_{p}X_{p} <0$\n",
    "\n",
    "then X lies on the other side of the hyperplane. So we can think of the hyperplane as dividing p-dimensional space into two halves. \n",
    "\n",
    "<img src=\"max.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "   \n",
    "The hyperplane 1 + 2X1 + 3X2 = 0 is shown. The blue region is the set of points for which 1 + 2X1 + 3X2 > 0, and the purple region is the set of points for which 1 + 2X1 + 3X2 < 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Using a Separating Hyperplane\n",
    "\n",
    "Our goal is to develop a classifier based on the training data that will correctly classify the test observation using its feature measurements.\n",
    "\n",
    "Suppose that it is possible to construct a hyperplane that separates the training observations perfectly according to their class labels. We can label the observations from the blue class as yi = 1 and those from the purple class as yi = −1. Then a separating hyperplane has the property that\n",
    "\n",
    "β0 +β1xi1 +β2xi2 +...+βpxip > 0 if yi = 1,\n",
    "\n",
    "and\n",
    "    \n",
    "β0 + β1xi1 + β2xi2 + . . . + βpxip < 0 if yi = −1.    \n",
    "\n",
    "Equivalently, a separating hyperplane has the property that\n",
    "\n",
    "yi(β0 + β1xi1 + β2xi2 + ... + βpxip) > 0\n",
    "\n",
    "for all i = 1,...,n.\n",
    "\n",
    "If a separating hyperplane exists, we can use it to construct a very natural\n",
    "classifier: a test observation is assigned a class depending on which side of\n",
    "the hyperplane it is located.\n",
    "\n",
    "<img src=\"svm.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "Left: There are two classes of observations, shown in blue and in purple, each of which has measurements on two variables. Three separating hyperplanes, out of many possible, are shown in black. Right: A separating hy- perplane is shown in black. The blue and purple grid indicates the decision rule made by a classifier based on this separating hyperplane: a test observation that falls in the blue portion of the grid will be assigned to the blue class, and a test observation that falls into the purple portion of the grid will be assigned to the purple class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Maximal Margin Classifier\n",
    "A natural choice is the maximal margin hyperplane (also known as the optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separat- ing hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin.\n",
    "\n",
    "The maximal margin hyperplane is the separating hyperplane for which the margin is largest—that is, it is the hyperplane that has the farthest minimum dis- tance to the training observations.\n",
    "    \n",
    "We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the maximal margin classifier. We hope that a classifier that has a large margin on the training data will also have a large margin on the test data, and hence will classify the test observations correctly. Although the maxi- mal margin classifier is often successful, it can also lead to overfitting when p is large.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes.\n",
    "\n",
    "<img src=\"svma.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "\n",
    "\n",
    "These three observations are known as support vectors,since they are vectors in p-dimensional space (in Figure, p = 2) and they “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyper- plane would move as well.\n",
    "\n",
    "\n",
    "    \n",
    "### Construction of the Maximal Margin Classifier\n",
    "\n",
    "We now consider the task of constructing the maximal margin hyperplane based on a set of n training observations $x1,...,xn ∈ R^{p}$ and associated class labels y1, . . . , yn ∈ {−1, 1}. Briefly, the maximal margin hyperplane is the solution to the optimization problem\n",
    "\n",
    "<img src=\"maxc.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "guarantees that each observation will be on the correct side of the hyper- plane, provided that M is positive. Hence, M represents the margin of our hyperplane, and the optimization problem chooses β0,β1,...,βp to maximize M. This is exactly the definition of the maximal margin hyperplane! The problem can be solved efficiently, but details of this optimization are outside of the scope of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Non-separable Case\n",
    "The maximal margin classifier is a very natural way to perform classifi- cation, if a separating hyperplane exists. However, as we have hinted, in many cases no separating hyperplane exists, and so there is no maximal margin classifier. In this case, the optimization problem has no solution with M > 0. An example is shown in Figure. In this case, we cannot exactly separate the two classes. \n",
    "\n",
    "However, as we will see in the next section, we can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier.\n",
    "\n",
    "<img src=\"non.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifiers\n",
    "\n",
    "The resulting maximal margin hyperplane is not satisfactory for one thing, it has only a tiny margin. This is problematic because as discussed previously, the distance of an observation from the hyperplane can be seen as a measure of our confidence that the observation was correctly classified. Moreover, the fact that the maximal mar- gin hyperplane is extremely sensitive to a change in a single observation suggests that it may have overfit the training data.\n",
    "\n",
    "The support vector classifier, sometimes called a soft margin classifier, does exactly this. Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. (The margin is soft because it can be violated by some of the training observations.) \n",
    "\n",
    "Most of the observations are on the correct side of the margin. However, a small subset of the observations are on the wrong side of the margin.\n",
    "<img src=\"svmb.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "In fact, when there is no separating hyperplane, such a situation is inevitable. Observations on the wrong side of the hyperplane correspond to training observations that are misclassified by the support vector classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the Support Vector Classifier\n",
    "\n",
    "The support vector classifier classifies a test observation depending on which side of a hyperplane it lies. The hyperplane is chosen to correctly support vector classifier soft margin classifier separate most of the training observations into the two classes, but may misclassify a few observations. It is the solution to the optimization problem\n",
    "\n",
    "<img src=\"svc.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n",
    "where C is a nonnegative tuning parameter. M is the width\n",
    "of the margin; we seek to make this quantity as large as possible. In (9.14),\n",
    "ε1, . . . , εn are slack variables that allow individual observations to be on\n",
    "the wrong side of the margin or the hyperplane; we will explain them in\n",
    "greater detail momentarily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If C = 0 then there is no budget for violations to the margin, and it must be the case that ε1 = ... = εn = 0, in which case svc simply amounts to the maximal margin hyperplane optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For C > 0 no more than C observa- tions can be on the wrong side of the hyperplane, because if an observation is on the wrong side of the hyperplane then εi > 1, and requires that 􏰂ni=1 εi ≤ C. As the budget C increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as C decreases, we become less tolerant of violations to the margin and so the margin narrows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is treated as a tuning parameter that is generally chosen via cross-validation. As with the tuning parameters that we have seen through- out this book, C controls the bias-variance trade-off of the statistical learn- ing technique. When C is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance. On the other hand, when C is larger, the margin is wider and we allow more violations to it; this amounts to fitting the data less hard and obtaining a classifier that is potentially more biased but may have lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "We first discuss a general mechanism for converting a linear classifier into one that produces non-linear decision boundaries. We then introduce the support vector machine, which does this in an automatic way.\n",
    "\n",
    "## Classification with Non-linear Decision Boundaries\n",
    "    \n",
    "The support vector classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear. How- ever, in practice we are sometimes faced with non-linear class boundaries.\n",
    "            \n",
    "We see there that the performance of linear regression can suffer when there is a non- linear relationship between the predictors and the outcome. In that case, we consider enlarging the feature space using functions of the predictors, such as quadratic and cubic terms, in order to address this non-linearity. In the case of the support vector classifier, we could address the prob- lem of possibly non-linear boundaries between classes in a similar way, by enlarging the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors. For instance, rather than fitting a support vector classifier using p features \n",
    "            \n",
    "we could instead fit a support vector classifier using 2p features\n",
    "\n",
    "$X_{1},X_{1}^{2},X_{2},X_{2}^{2},...,X_{p},X_{p}^{2}$\n",
    "\n",
    "<img src=\"svmr.png\" alt=\"(Screenshot of wikipedia table of WA EVD cases)\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
